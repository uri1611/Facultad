{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32aac22e-94d0-46a4-b672-d7f1656782ac",
      "metadata": {
        "id": "32aac22e-94d0-46a4-b672-d7f1656782ac"
      },
      "source": [
        "#  <center> Taller  de Aprendizaje Automático </center>\n",
        "##  <center> PROYECTO 2: Freesound Audio Tagging 2019  </center>\n",
        "###  <center> Grupo C  </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12125846",
      "metadata": {
        "id": "12125846",
        "outputId": "1a081126-5ac9-46fa-d32e-3070be70ae0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ureca\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
            "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import wave\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import resample\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pydub\n",
        "from pydub.silence import split_on_silence\n",
        "from pydub import AudioSegment, effects\n",
        "from scipy.io.wavfile import read, write\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from comet_ml import Experiment\n",
        "from tensorflow.keras.models import load_model # model = load_model(\"modelo_final.keras\", custom_objects={'LWLRAP': LWLRAP})\n",
        "import kapre\n",
        "from spec_augment import SpecAugment # https://github.com/MichaelisTrofficus/spec_augment\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f6d6d0",
      "metadata": {
        "id": "a4f6d6d0"
      },
      "source": [
        "# Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69a9a72",
      "metadata": {
        "id": "e69a9a72"
      },
      "outputs": [],
      "source": [
        "# df_train_curados = pd.read_csv('train_curated.csv')\n",
        "# df_train_ruidosos = pd.read_csv('train_noisy.csv')\n",
        "\n",
        "df_train_curados = pd.read_csv('train_curated.csv')\n",
        "df_train_ruidosos = pd.read_csv('train_noisy.csv')\n",
        "\n",
        "df_test = pd.read_csv('sample_submission_v24.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736765fc",
      "metadata": {
        "id": "736765fc"
      },
      "source": [
        "# Funciones de preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f343cc6",
      "metadata": {
        "id": "2f343cc6"
      },
      "source": [
        "## De carpetas\n",
        "\n",
        "Se dejan por si las quieren ver pero se ejecutaron sobre las carpetas no en este notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8a40d2",
      "metadata": {
        "id": "8d8a40d2"
      },
      "outputs": [],
      "source": [
        "# sacar silencios\n",
        "# https://onkar-patil.medium.com/how-to-remove-silence-from-an-audio-using-python-50fd2c00557d\n",
        "\n",
        "def sacar_silencios(ruta_audio, nombre,path_salida, largo_optimo = 1000):\n",
        "    ''''\n",
        "    ruta_audio: ruta del audio a recortar\n",
        "    largo_optimo: largo en mili segundos a considerar\n",
        "    '''\n",
        "\n",
        "    rate, audio = read(ruta_audio)\n",
        "\n",
        "    largo_audio = len(audio)/rate *1000\n",
        "\n",
        "    largo = min(largo_optimo,largo_audio) # se usa solo para curados\n",
        "\n",
        "    aud = AudioSegment(audio.tobytes(),frame_rate = rate,\n",
        "                        sample_width = audio.dtype.itemsize,channels = 1)\n",
        "\n",
        "    audio_chunks = split_on_silence(\n",
        "        aud,\n",
        "        # USADO PARA CURADOS\n",
        "        # min_silence_len = int(largo/10), # evaluar si 1/4 del largo optimo esta ok o no. a lo mejor\n",
        "        # silence_thresh = -45, # umbral de silencio.. averiguar ruidos tipicos\n",
        "        # keep_silence = largo/100,) # o menos? ver que sea en funcion del largo del audio\n",
        "\n",
        "        # USADO PARA RUIDOSOS\n",
        "        min_silence_len=300,\n",
        "        silence_thresh=-40,\n",
        "        keep_silence=100)\n",
        "\n",
        "\n",
        "    # esto es para cuando todo es detectado como silencio\n",
        "    if len(audio_chunks) == 0:\n",
        "        audio_chunks = [aud]\n",
        "    audio_processed = sum(audio_chunks)\n",
        "    audio_processed = np.array(audio_processed.get_array_of_samples())\n",
        "\n",
        "    ruta_audio_salida = path_salida + nombre\n",
        "\n",
        "    write(ruta_audio_salida,rate,audio_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0565cd3f",
      "metadata": {
        "id": "0565cd3f"
      },
      "outputs": [],
      "source": [
        "def resampleo(ruta_audio,nombre,path_salida):\n",
        "    rate, audio = read(ruta_audio)\n",
        "    origin_num_samples = len(audio)\n",
        "    new_samps = int(origin_num_samples * 16000/rate)\n",
        "\n",
        "    # resampling\n",
        "    target_audio_scipy = resample(audio, new_samps).astype(int)\n",
        "    target_audio_scipy = np.array(target_audio_scipy, np.int16)\n",
        "\n",
        "    ruta_audio_salida = path_salida + nombre # evaluar nombre y si lo guardamos en la misma carpeta\n",
        "\n",
        "    write(ruta_audio_salida,16000,target_audio_scipy)\n",
        "    return target_audio_scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12359f0d",
      "metadata": {
        "id": "12359f0d"
      },
      "source": [
        "## Del pipeline - TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "502fa156",
      "metadata": {
        "id": "502fa156"
      },
      "outputs": [],
      "source": [
        "# llevar todos los audios al mismo largo -> estrategia: repetir secuencia hasta obtener largo definido como óptimo\n",
        "# cortar los largos para que en cada batch elija una parte diferente cada vez\n",
        "\n",
        "def energia(waveform):\n",
        "    '''\n",
        "    Calcula la energía de un waveform como la suma de sus muestras al cuadrado\n",
        "    '''\n",
        "    return tf.reduce_sum(tf.square(waveform))\n",
        "\n",
        "\n",
        "def largo_fijo(ruta_audio, largo_optimo = 3):\n",
        "    ''''\n",
        "    ruta_audio: ruta del audio a recortar\n",
        "    largo_optimo: largo en segundos a considerar\n",
        "    '''\n",
        "    file = tf.io.read_file(ruta_audio)\n",
        "    waveform, sample_rate = tf.audio.decode_wav(file)  # sample_rate correcto\n",
        "\n",
        "    waveform = tf.squeeze(waveform, axis=-1)  #  1D\n",
        "\n",
        "    wav_len = tf.shape(waveform)[0]\n",
        "    largo_optimo_samples = tf.cast(sample_rate * largo_optimo, tf.int32)\n",
        "\n",
        "    if tf.less(wav_len, largo_optimo_samples):\n",
        "        # print('entro bien (audio corto)')\n",
        "        reps = tf.cast(\n",
        "            tf.math.ceil(tf.cast(largo_optimo_samples, tf.float32) / tf.cast(wav_len, tf.float32)),\n",
        "            tf.int32\n",
        "        )\n",
        "        waveform = tf.tile(waveform, [reps])\n",
        "        waveform = waveform[:largo_optimo_samples]\n",
        "\n",
        "    elif tf.greater(wav_len, largo_optimo_samples):\n",
        "        inicio = tf.random.uniform([], 0, wav_len - largo_optimo_samples + 1, dtype=tf.int32, seed=42)\n",
        "        waveform_i = waveform[inicio:inicio + largo_optimo_samples]\n",
        "        for _ in range(2):\n",
        "          inicio2 = tf.random.uniform([], 0, wav_len - largo_optimo_samples + 1, dtype=tf.int32, seed=42)\n",
        "          waveform2 = waveform[inicio2:inicio2 + largo_optimo_samples]\n",
        "          if energia(waveform2) > energia(waveform_i):\n",
        "              waveform_i = waveform2\n",
        "\n",
        "        waveform = waveform_i  # usar el segmento de mayor energía\n",
        "\n",
        "    # Si es igual, no hago nada\n",
        "    # else:\n",
        "    #     print('dura 1 segundo')\n",
        "\n",
        "    return waveform, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d555abd0",
      "metadata": {
        "id": "d555abd0"
      },
      "outputs": [],
      "source": [
        "def audio_a_imagen(waveform, sample_rate):\n",
        "    frame_length = 1024\n",
        "    frame_step = 512\n",
        "    window_fn = tf.signal.hann_window\n",
        "    fft_length = 1024\n",
        "\n",
        "    stft = tf.signal.stft(waveform,frame_length = frame_length,frame_step = frame_step,fft_length = fft_length,window_fn = window_fn)\n",
        "    spectogram = tf.abs(stft)\n",
        "    num_spectogram_bins = stft.shape[-1]\n",
        "\n",
        "    lower_edge_hertz, upper_edge_hertz, num_mel_bins= 0, 4000, 80 # evaluar valor frec max por largo imagen\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins,num_spectogram_bins,sample_rate,lower_edge_hertz,upper_edge_hertz)\n",
        "\n",
        "    # se aplica el banco de filtros sobre el espectograma\n",
        "    mel_spectograms = tf.tensordot(spectogram,linear_to_mel_weight_matrix,1)\n",
        "    mel_spectograms.set_shape(spectogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
        "\n",
        "    # calculo el espectograma en magnitud logaritmica y escala mel\n",
        "    log_mel_spectograms = tf.math.log(mel_spectograms+1e-6)\n",
        "\n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectograms)[...,:13]\n",
        "\n",
        "    ##################### usamos valores generales de log mel (buscar bibliografia de referencia)\n",
        "\n",
        "    log_mel_db = 20 * log_mel_spectograms / tf.math.log(10.0) # lo llevo a decibeles para usar valores de referencia\n",
        "\n",
        "    min_val = -80\n",
        "    max_val = 0\n",
        "    log_mel_db = tf.clip_by_value(log_mel_db, min_val, max_val)  # llevo todo al rango -80 0\n",
        "    log_mel_reescalado = (log_mel_db - min_val) / (max_val - min_val) * 255.0 # llevo todo a 0 - 255\n",
        "\n",
        "    return log_mel_reescalado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bad9df5",
      "metadata": {
        "id": "9bad9df5"
      },
      "outputs": [],
      "source": [
        "def codificar_etiquetas(df, columna='labels'):\n",
        "    # todas las etiquetas ordenadas alfabeticamente\n",
        "    all_labels = sorted(set(','.join(df[columna]).split(',')))\n",
        "    label_to_index = {label: i for i, label in enumerate(all_labels)}\n",
        "    num_classes = len(all_labels)\n",
        "    # funcion adaptada del ejemplo\n",
        "    def encode_labels(label_str):\n",
        "        vec = np.zeros(num_classes, dtype=np.float32)\n",
        "        for label in label_str.split(','):\n",
        "            vec[label_to_index[label]] = 1.0\n",
        "        return vec\n",
        "    # mapeo\n",
        "    df = df.copy()\n",
        "    df['label_vec'] = df[columna].map(encode_labels)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d851fe0",
      "metadata": {
        "id": "4d851fe0"
      },
      "source": [
        "# METRICA\n",
        "\n",
        "https://www.kaggle.com/code/carlthome/l-lrap-metric-for-tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24d0d82",
      "metadata": {
        "id": "e24d0d82"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "\n",
        "def _lwlrap_sklearn(truth, scores):\n",
        "    \"\"\"Reference implementation from https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8\"\"\"\n",
        "    sample_weight = np.sum(truth > 0, axis=1)\n",
        "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
        "    overall_lwlrap = label_ranking_average_precision_score(\n",
        "        truth[nonzero_weight_sample_indices, :] > 0,\n",
        "        scores[nonzero_weight_sample_indices, :],\n",
        "        sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
        "    return overall_lwlrap\n",
        "\n",
        "\n",
        "def _one_sample_positive_class_precisions(example):\n",
        "    y_true, y_pred = example\n",
        "\n",
        "    retrieved_classes = tf.argsort(y_pred, direction='DESCENDING')\n",
        "    class_rankings = tf.argsort(retrieved_classes)\n",
        "    retrieved_class_true = tf.gather(y_true, retrieved_classes)\n",
        "    retrieved_cumulative_hits = tf.math.cumsum(tf.cast(retrieved_class_true, tf.float32))\n",
        "\n",
        "    idx = tf.where(y_true)[:, 0]\n",
        "    i = tf.boolean_mask(class_rankings, y_true)\n",
        "    r = tf.gather(retrieved_cumulative_hits, i)\n",
        "    c = 1 + tf.cast(i, tf.float32)\n",
        "    precisions = r / c\n",
        "\n",
        "    dense = tf.scatter_nd(idx[:, None], precisions, [y_pred.shape[0]])\n",
        "    return dense\n",
        "\n",
        "class LWLRAP(tf.keras.metrics.Metric):\n",
        "    def __init__(self, num_classes, name='lwlrap', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self._precisions = self.add_weight(\n",
        "            name='per_class_cumulative_precision',\n",
        "            shape=[num_classes],\n",
        "            initializer='zeros',\n",
        "        )\n",
        "\n",
        "        self._counts = self.add_weight(\n",
        "            name='per_class_cumulative_count',\n",
        "            shape=[num_classes],\n",
        "            initializer='zeros',\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        precisions = tf.map_fn(\n",
        "            fn=_one_sample_positive_class_precisions,\n",
        "            elems=(y_true, y_pred),\n",
        "            dtype=(tf.float32),\n",
        "        )\n",
        "\n",
        "        increments = tf.cast(precisions > 0, tf.float32)\n",
        "        total_increments = tf.reduce_sum(increments, axis=0)\n",
        "        total_precisions = tf.reduce_sum(precisions, axis=0)\n",
        "\n",
        "        self._precisions.assign_add(total_precisions)\n",
        "        self._counts.assign_add(total_increments)\n",
        "\n",
        "    def result(self):\n",
        "        per_class_lwlrap = self._precisions / tf.maximum(self._counts, 1.0)\n",
        "        per_class_weight = self._counts / tf.reduce_sum(self._counts)\n",
        "        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * per_class_weight)\n",
        "        return overall_lwlrap\n",
        "\n",
        "    def reset_states(self):\n",
        "        self._precisions.assign(tf.zeros_like(self._precisions))\n",
        "        self._counts.assign(tf.zeros_like(self._counts))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_classes': self.num_classes,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def test_match_sklearn():\n",
        "\n",
        "    # Generate dummy data like https://colab.research.google.com/drive/1YwL7ewUE6vSLZRoSf3Oi9efyupLyfcdb\n",
        "    num_samples = 100\n",
        "    num_labels = 20\n",
        "    scores = np.random.rand(num_samples, num_labels)\n",
        "    truth = np.random.rand(num_samples, num_labels) > 0.5\n",
        "    truth[0:1, :] = False\n",
        "\n",
        "    # Compute expected overall LWLRAP.\n",
        "    desired = _lwlrap_sklearn(truth, scores)\n",
        "\n",
        "    # Accumulate LWLRAP per minibatch with tf.metrics.Metric.\n",
        "    metric = LWLRAP(num_labels)\n",
        "    batch_size = 1\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        y_true = truth[i:i+batch_size]\n",
        "        y_pred = scores[i:i+batch_size]\n",
        "        x = metric.update_state(y_true, y_pred)\n",
        "    actual = metric.result()\n",
        "\n",
        "    # Make sure both methods get similar averages.\n",
        "    np.testing.assert_allclose(actual, desired)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_match_sklearn()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bf1fb4",
      "metadata": {
        "id": "e9bf1fb4"
      },
      "source": [
        "# FUNCIONES QUE SE USAN EN LOS MODELOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10ad34a",
      "metadata": {
        "id": "e10ad34a"
      },
      "outputs": [],
      "source": [
        "#### CONVOLUCIONAL SENCILLO\n",
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "\n",
        "    x = layers.BatchNormalization()(inputs)\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)  # <-- esta capa aplana sin romper la forma\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)  # sin activación, logits\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "#### CONVOLUCIONAL SENCILLO ADAPTADO PARA SER CABEZA\n",
        "\n",
        "def make_model_cabeza(x, num_classes):\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "    return outputs\n",
        "\n",
        "#### RESNET CON CONVOLUCIONAL SENCILLO O DENSAS\n",
        "\n",
        "def make_resnet50_model_cabeza(input_shape=(224, 224, 3), num_classes=80,capas_entrenables=0,cabeza_basica=False):\n",
        "    base_model = ResNet50(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=input_shape,\n",
        "        pooling= None,\n",
        "    )\n",
        "    if capas_entrenables == 0:\n",
        "        base_model.trainable = False  # Congela pesos\n",
        "    else:\n",
        "        base_model.trainable = True # habilito todo\n",
        "        for layer in base_model.layers[:-capas_entrenables]: # deshabilito las ultimas\n",
        "            layer.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # x = layers.Rescaling(1./255)(inputs)  # Normalizar\n",
        "    x = base_model(inputs, training=(capas_entrenables > 0)) # si entreno capas entonces quiero que haya entrenamiento y da True sino da false\n",
        "    if cabeza_basica:\n",
        "        x = layers.GlobalAveragePooling2D()(x)  # pasar a 2D\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        outputs = layers.Dense(num_classes, activation='sigmoid')(x)  # Multilabel\n",
        "    else:\n",
        "        outputs= make_model_cabeza(x, num_classes)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "#### MOBILNET CON CONVOLUCIONAL SENCILLO O DENSAS\n",
        "\n",
        "def make_mobilenet_model_cabeza(input_shape=(224, 224, 3), num_classes=80, capas_entrenables=0, cabeza_basica=False):\n",
        "    base_model = MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=input_shape,\n",
        "        pooling=None\n",
        "    )\n",
        "    if capas_entrenables == 0:\n",
        "        base_model.trainable = False  # Congela pesos\n",
        "    else:\n",
        "        base_model.trainable = True # habilito todo\n",
        "        for layer in base_model.layers[:-capas_entrenables]: # deshabilito las ultimas\n",
        "            layer.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    # x = layers.Rescaling(1./255)(inputs)  # Normalizar\n",
        "    x = base_model(inputs, training=(capas_entrenables > 0)) # si entreno capas entonces quiero que haya entrenamiento y da True sino da false\n",
        "    if cabeza_basica:\n",
        "        x = layers.GlobalAveragePooling2D()(x)  # pasar a 2D\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        outputs = layers.Dense(num_classes, activation='sigmoid')(x)  # Multilabel\n",
        "    else:\n",
        "        outputs= make_model_cabeza(x, num_classes)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7cf87af",
      "metadata": {
        "id": "b7cf87af"
      },
      "outputs": [],
      "source": [
        "# Preprocesamiento basico\n",
        "\n",
        "def preprocesamiento(ruta_audio,label): # ojo que esta funcion no deja pasar parametros de las funciones que tiene dentro\n",
        "    '''\n",
        "    ruta_carpeta: ruta a la carpeta con wav  in silencios y resampleada\n",
        "    label: posta 80\n",
        "    '''\n",
        "    waveform, sample_rate = largo_fijo(ruta_audio)\n",
        "    log_mel = audio_a_imagen(waveform, sample_rate)\n",
        "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
        "    return log_mel,label\n",
        "\n",
        "#### preprocesamiento RESNET\n",
        "\n",
        "\n",
        "def preprocesamiento3canalesresnet(ruta_audio, label):\n",
        "    waveform, sample_rate = largo_fijo(ruta_audio)\n",
        "    log_mel = audio_a_imagen(waveform, sample_rate)\n",
        "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
        "    log_mel = tf.image.grayscale_to_rgb(log_mel)\n",
        "    log_mel = tf.image.resize_with_pad(log_mel, 224, 224)\n",
        "\n",
        "    log_mel = tf.keras.applications.resnet50.preprocess_input(log_mel)\n",
        "    return log_mel, label\n",
        "\n",
        "#### preprocesamiento MobilNet\n",
        "\n",
        "def preprocesamiento3canalesmobile(ruta_audio, label):\n",
        "    waveform, sample_rate = largo_fijo(ruta_audio)\n",
        "    log_mel = audio_a_imagen(waveform, sample_rate)\n",
        "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
        "    log_mel = tf.image.grayscale_to_rgb(log_mel)\n",
        "    log_mel = tf.image.resize_with_pad(log_mel, 224, 224)\n",
        "\n",
        "    log_mel = tf.keras.applications.mobilenet_v2.preprocess_input(log_mel)\n",
        "    return log_mel, label\n",
        "\n",
        "\n",
        "spec_augment = SpecAugment(freq_mask_param=5,\n",
        "                           time_mask_param=5,\n",
        "                           n_freq_mask=1,\n",
        "                           n_time_mask=1,\n",
        "                           mask_value=0)\n",
        "\n",
        "#### preprocesamiento basico aumentado con SpecAugment\n",
        "\n",
        "def preprocesamiento_aumentado(ruta_audio,label): # ojo que esta funcion no deja pasar parametros de las funciones que tiene dentro\n",
        "    '''\n",
        "    ruta_carpeta: ruta a la carpeta con wav sin silencios y resampleada\n",
        "    label: posta 80\n",
        "    '''\n",
        "    log_mel, label = preprocesamiento(ruta_audio,label) # genero log_mel\n",
        "    log_mel_aumentado = spec_augment(tf.expand_dims(log_mel, 0), training=True) # agrego dim extra para q ande\n",
        "    log_mel_aumentado_reescalado = tf.squeeze(log_mel_aumentado, axis=0)  # saco dim extra\n",
        "\n",
        "    return log_mel_aumentado_reescalado,label\n",
        "\n",
        "#### preprocesamiento ResNet aumentado con SpecAugment\n",
        "\n",
        "def preprocesamiento3canalesresnet_aumentado(ruta_audio, label):\n",
        "\n",
        "    log_mel, label = preprocesamiento3canalesresnet(ruta_audio,label) # genero log_mel\n",
        "    log_mel_aumentado = spec_augment(tf.expand_dims(log_mel, 0), training=True) # agrego dim extra para q ande\n",
        "    log_mel_aumentado_reescalado = tf.squeeze(log_mel_aumentado, axis=0)  # saco dim extra\n",
        "\n",
        "    return log_mel_aumentado_reescalado,label\n",
        "\n",
        "#### preprocesamiento MobilNet aumentado con SpecAugment\n",
        "\n",
        "def preprocesamiento3canalesmobile_aumentado(ruta_audio, label):\n",
        "\n",
        "    log_mel, label = preprocesamiento3canalesmobile(ruta_audio,label) # genero log_mel\n",
        "    log_mel_aumentado = spec_augment(tf.expand_dims(log_mel, 0), training=True) # agrego dim extra para q ande\n",
        "    log_mel_aumentado_reescalado = tf.squeeze(log_mel_aumentado, axis=0)  # saco dim extra\n",
        "\n",
        "    return log_mel_aumentado_reescalado,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646baeaa",
      "metadata": {
        "id": "646baeaa"
      },
      "outputs": [],
      "source": [
        "def crear_datasets_train_val(df,carpeta_audio='train_curated_sin_silencios',prep=None,batch_size=32,ratio=0.2,shuffle=True):\n",
        "    # \"\"\"\n",
        "    # prep es comun o resnet\n",
        "    # \"\"\"\n",
        "    df_train_, df_val_ = train_test_split(df, test_size=ratio, shuffle=shuffle, random_state=42)\n",
        "\n",
        "    df_train_80 = codificar_etiquetas(df_train_)\n",
        "    df_val_80 = codificar_etiquetas(df_val_)\n",
        "\n",
        "    carpeta_audio = carpeta_audio+'/' # ojo que acá se agrega la barra\n",
        "\n",
        "    file_paths_train = [carpeta_audio + fname for fname in df_train_80['fname']]\n",
        "    file_paths_val = [carpeta_audio + fname for fname in df_val_80['fname']]\n",
        "\n",
        "    label_vectors_train = df_train_80['label_vec'].tolist()\n",
        "    label_vectors_val   = df_val_80['label_vec'].tolist()\n",
        "\n",
        "    ds_train = tf.data.Dataset.from_tensor_slices((file_paths_train, label_vectors_train))\n",
        "    ds_val   = tf.data.Dataset.from_tensor_slices((file_paths_val, label_vectors_val))\n",
        "\n",
        "    if prep == 'comun':\n",
        "        ds_train = ds_train.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'resnet':\n",
        "            ds_train = ds_train.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_val   = ds_val.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'mobile':\n",
        "        ds_train = ds_train.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    ds_val   = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_train, ds_val\n",
        "\n",
        "\n",
        "def crear_datasets_train_val_strat(df, carpeta_audio='train_curated_sin_silencios_resample', prep=None, batch_size=32, ratio=0.2):\n",
        "    # \"\"\"\n",
        "    # prep es comun o resnet\n",
        "    # \"\"\"\n",
        "    df_codificado = codificar_etiquetas(df)\n",
        "\n",
        "    X = df_codificado['fname'].values.reshape(-1, 1)  # reshape porque X debe ser 2D\n",
        "    y = np.stack(df_codificado['label_vec'].values)   # convertir lista de vectores a array\n",
        "\n",
        "    X_train, y_train, X_val, y_val = iterative_train_test_split(X, y, test_size=ratio)\n",
        "\n",
        "    # Reconstruir DataFrames\n",
        "    df_train_ = pd.DataFrame({'fname': X_train.ravel(), 'label_vec': list(y_train)})\n",
        "    df_val_   = pd.DataFrame({'fname': X_val.ravel(),   'label_vec': list(y_val)})\n",
        "\n",
        "    carpeta_audio = carpeta_audio + '/'\n",
        "\n",
        "    file_paths_train = [carpeta_audio + fname for fname in df_train_['fname']]\n",
        "    file_paths_val   = [carpeta_audio + fname for fname in df_val_['fname']]\n",
        "\n",
        "    ds_train = tf.data.Dataset.from_tensor_slices((file_paths_train, df_train_['label_vec'].tolist()))\n",
        "    ds_val   = tf.data.Dataset.from_tensor_slices((file_paths_val, df_val_['label_vec'].tolist()))\n",
        "\n",
        "    if prep == 'comun':\n",
        "        ds_train = ds_train.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'resnet':\n",
        "        ds_train = ds_train.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'mobile':\n",
        "        ds_train = ds_train.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'modelo3':\n",
        "        ds_train = ds_train.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    ds_val   = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_train, ds_val\n",
        "\n",
        "def crear_datasets_train_val_aumentado_strat(df,carpeta_audio=None,prep=None,batch_size=32,ratio=0.2):\n",
        "    # \"\"\"\n",
        "    # prep es comun o resnet\n",
        "    # \"\"\"\n",
        "    df_codificado = codificar_etiquetas(df)\n",
        "\n",
        "    X = df_codificado['fname'].values.reshape(-1, 1)  # reshape porque X debe ser 2D\n",
        "    y = np.stack(df_codificado['label_vec'].values)   # convertir lista de vectores a array\n",
        "\n",
        "    X_train, y_train, X_val, y_val = iterative_train_test_split(X, y, test_size=ratio)\n",
        "\n",
        "    # Reconstruir DataFrames\n",
        "    df_train_ = pd.DataFrame({'fname': X_train.ravel(), 'label_vec': list(y_train)})\n",
        "    df_val_   = pd.DataFrame({'fname': X_val.ravel(),   'label_vec': list(y_val)})\n",
        "\n",
        "    carpeta_audio = carpeta_audio+'/' # ojo que acá se agrega la barra\n",
        "\n",
        "    file_paths_train = [carpeta_audio + fname for fname in df_train_['fname']]\n",
        "    file_paths_val = [carpeta_audio + fname for fname in df_val_['fname']]\n",
        "\n",
        "    label_vectors_train = df_train_['label_vec'].tolist()\n",
        "    label_vectors_val   = df_val_['label_vec'].tolist()\n",
        "\n",
        "    ds_train = tf.data.Dataset.from_tensor_slices((file_paths_train, label_vectors_train))\n",
        "    ds_train_aumentado = tf.data.Dataset.from_tensor_slices((file_paths_train, label_vectors_train))\n",
        "    ds_val   = tf.data.Dataset.from_tensor_slices((file_paths_val, label_vectors_val))\n",
        "\n",
        "\n",
        "    if prep == 'comun':\n",
        "        ds_train = ds_train.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_train_aumentado = ds_train_aumentado.map(preprocesamiento_aumentado, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.map(preprocesamiento, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'resnet':\n",
        "            ds_train = ds_train.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_train_aumentado = ds_train_aumentado.map(preprocesamiento3canalesresnet_aumentado, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_val   = ds_val.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'mobile':\n",
        "            ds_train = ds_train.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_train_aumentado = ds_train_aumentado.map(preprocesamiento3canalesmobile_aumentado, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_val   = ds_val.map(preprocesamiento3canalesmobile, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    elif prep == 'modelo3':\n",
        "            ds_train = ds_train.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            ds_train_aumentado = ds_train_aumentado.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "            mixup_layer = tf.keras.layers.MixUp(alpha=0.3)\n",
        "\n",
        "            def to_dict(x, y):\n",
        "                return {\"images\": x, \"labels\": y}\n",
        "\n",
        "            def from_dict(batch):\n",
        "                return batch[\"images\"], batch[\"labels\"]\n",
        "\n",
        "            ds_train_aumentado = ds_train_aumentado.batch(batch_size)\n",
        "            ds_train_aumentado = ds_train_aumentado.map(to_dict)\n",
        "            ds_train_aumentado = ds_train_aumentado.map(lambda batch: mixup_layer(batch, training=True))\n",
        "            ds_train_aumentado = ds_train_aumentado.map(from_dict)\n",
        "            ds_train_aumentado = ds_train_aumentado.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "            ds_val   = ds_val.map(preprocesamiento3canalesresnet, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "            ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "            ds_train = ds_train.concatenate(ds_train_aumentado)\n",
        "            ds_val   = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if prep != 'modelo3':\n",
        "        ds_train = ds_train.concatenate(ds_train_aumentado) # junto los dos train\n",
        "\n",
        "        ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "        ds_val   = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds_train, ds_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d0f061",
      "metadata": {
        "id": "b6d0f061"
      },
      "outputs": [],
      "source": [
        "def entrenar_para_test(modelo, df_train, path_audios, batch_size=128, epocas=50, df_val=None):\n",
        "    \"\"\"\n",
        "    Dejo la opcion de val por si usamos earlystopping o similar\n",
        "    \"\"\"\n",
        "\n",
        "    file_paths_train = [path_audios + fname for fname in df_train['fname']]\n",
        "\n",
        "    df_train_80 = codificar_etiquetas(df_train)\n",
        "    label_vectors_train = df_train_80['label_vec'].tolist()\n",
        "\n",
        "    ds_train = tf.data.Dataset.from_tensor_slices((file_paths_train, label_vectors_train))\n",
        "    ds_train = ds_train.map(lambda path, label: preprocesamiento(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if df_val is not None:\n",
        "        file_paths_val = [path_audios + fname for fname in df_val['fname']]\n",
        "        df_val_80 = codificar_etiquetas(df_val)\n",
        "        label_vectors_val = df_val_80['label_vec'].tolist()\n",
        "\n",
        "        ds_val = tf.data.Dataset.from_tensor_slices((file_paths_val, label_vectors_val))\n",
        "        ds_val = ds_val.map(lambda path, label: preprocesamiento(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        history = modelo.fit(ds_train, validation_data=ds_val, epochs=epocas)\n",
        "    else:\n",
        "        history = modelo.fit(ds_train, epochs=epocas)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def entrenar_para_test_resnet(modelo, df_train, path_audios, batch_size=128, epocas=50, df_val=None):\n",
        "    \"\"\"\n",
        "    Dejo la opcion de val por si usamos earlystopping o similar\n",
        "    \"\"\"\n",
        "\n",
        "    file_paths_train = [path_audios + fname for fname in df_train['fname']]\n",
        "\n",
        "    df_train_80 = codificar_etiquetas(df_train)\n",
        "    label_vectors_train = df_train_80['label_vec'].tolist()\n",
        "\n",
        "    ds_train = tf.data.Dataset.from_tensor_slices((file_paths_train, label_vectors_train))\n",
        "    ds_train = ds_train.map(lambda path, label: preprocesamiento3canalesresnet(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    if df_val is not None:\n",
        "        file_paths_val = [path_audios + fname for fname in df_val['fname']]\n",
        "        df_val_80 = codificar_etiquetas(df_val)\n",
        "        label_vectors_val = df_val_80['label_vec'].tolist()\n",
        "\n",
        "        ds_val = tf.data.Dataset.from_tensor_slices((file_paths_val, label_vectors_val))\n",
        "        ds_val = ds_val.map(lambda path, label: preprocesamiento3canalesresnet(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        history = modelo.fit(ds_train, validation_data=ds_val, epochs=epocas)\n",
        "    else:\n",
        "        history = modelo.fit(ds_train, epochs=epocas)\n",
        "\n",
        "    return history\n",
        "\n",
        "def generar_submission_resnet(modelo, df_test, path_salida='submission.csv'):\n",
        "\n",
        "    file_paths_test = ['test_sin_silencios_resample/' + fname for fname in df_test['fname'].tolist()]\n",
        "    dummy_labels = [tf.zeros((80,))] * len(file_paths_test)\n",
        "\n",
        "    ds_test = tf.data.Dataset.from_tensor_slices((file_paths_test, dummy_labels))\n",
        "    ds_test = ds_test.map(lambda path, label: preprocesamiento3canalesresnet(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_test = ds_test.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    predicciones = modelo.predict(ds_test)\n",
        "\n",
        "    labels = df_test.columns[1:].tolist()\n",
        "    df_submission = pd.DataFrame(predicciones, columns=labels)\n",
        "    df_submission.insert(0, \"fname\", df_test[\"fname\"])\n",
        "    df_submission.to_csv(path_salida, index=False)\n",
        "\n",
        "    return df_submission\n",
        "\n",
        "def generar_submission_mobile(modelo, df_test, path_salida='submission.csv'):\n",
        "\n",
        "    file_paths_test = ['test_sin_silencios_resample/' + fname for fname in df_test['fname'].tolist()]\n",
        "    dummy_labels = [tf.zeros((80,))] * len(file_paths_test)\n",
        "\n",
        "    ds_test = tf.data.Dataset.from_tensor_slices((file_paths_test, dummy_labels))\n",
        "    ds_test = ds_test.map(lambda path, label: preprocesamiento3canalesmobile(path, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds_test = ds_test.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    predicciones = modelo.predict(ds_test)\n",
        "\n",
        "    labels = df_test.columns[1:].tolist()\n",
        "    df_submission = pd.DataFrame(predicciones, columns=labels)\n",
        "    df_submission.insert(0, \"fname\", df_test[\"fname\"])\n",
        "    df_submission.to_csv(path_salida, index=False)\n",
        "\n",
        "    return df_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dd7ad8",
      "metadata": {
        "id": "11dd7ad8"
      },
      "outputs": [],
      "source": [
        "def cargar_a_comet(history, exp, optimizer='adam', epochs=50):\n",
        "    '''\n",
        "    Ver si queremos cargar el lr, como lo vamos cambiando lo saqué\n",
        "    '''\n",
        "    train_loss = history.history.get(\"loss\")\n",
        "    train_acc = history.history.get(\"acc\")\n",
        "    train_lwlrap = history.history.get(\"lwlrap\")\n",
        "\n",
        "    val_loss = history.history.get(\"val_loss\")\n",
        "    val_acc = history.history.get(\"val_acc\")\n",
        "    val_lwlrap = history.history.get(\"val_lwlrap\")\n",
        "\n",
        "    if val_lwlrap:\n",
        "        best_epoch = np.argmax(val_lwlrap)\n",
        "    else:\n",
        "        best_epoch = np.argmin(val_loss)\n",
        "\n",
        "    best_train_loss = train_loss[best_epoch] if train_loss else None\n",
        "    best_train_acc = train_acc[best_epoch] if train_acc else None\n",
        "    best_train_lwlrap = train_lwlrap[best_epoch] if train_lwlrap else None\n",
        "\n",
        "    best_val_loss = val_loss[best_epoch] if val_loss else None\n",
        "    best_val_acc = val_acc[best_epoch] if val_acc else None\n",
        "    best_val_lwlrap = val_lwlrap[best_epoch] if val_lwlrap else None\n",
        "\n",
        "    for metric_name, values in history.history.items():\n",
        "        for epoch, value in enumerate(values):\n",
        "            exp.log_metric(metric_name, value, step=epoch)\n",
        "\n",
        "    exp.log_metrics({\n",
        "        \"best_train_loss\": best_train_loss,\n",
        "        \"best_train_acc\": best_train_acc,\n",
        "        \"best_train_lwlrap\": best_train_lwlrap,\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"best_val_lwlrap\": best_val_lwlrap,\n",
        "    })\n",
        "\n",
        "    exp.log_parameters({\n",
        "        \"optimizer\": optimizer,\n",
        "        \"loss\": 'binary_crossentropy',\n",
        "        \"epochs\": epochs\n",
        "    })\n",
        "\n",
        "    exp.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b369c01",
      "metadata": {
        "id": "9b369c01"
      },
      "source": [
        "# Modelos en test\n",
        "\n",
        "- Resnet entrenando ultimos dos bloques\n",
        "- Mobilnet entrenado completamente\n",
        "\n",
        "Ambos modelos finalizan con la convolucional sencilla"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cc9e66a",
      "metadata": {
        "id": "0cc9e66a"
      },
      "source": [
        "## Resnet entrenando ultimos dos bloques\n",
        "\n",
        "Si se va a correr considerar cambiar path a archivos al llamar función de crear datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26842ab",
      "metadata": {
        "id": "d26842ab"
      },
      "outputs": [],
      "source": [
        "ds_train_ruidosos_resnet, ds_val_ruidosos_resnet = crear_datasets_train_val(df_train_ruidosos,carpeta_audio='train_ruidosos_sin_silencios_resample',prep='resnet',batch_size=32,ratio=0.1,shuffle=True)\n",
        "ds_train_curados_resnet, ds_val_curados_resnet = crear_datasets_train_val_aumentado_strat(df_train_curados,carpeta_audio='train_curated_sin_silencios_resample',prep='resnet',batch_size=32,ratio=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f70f1a",
      "metadata": {
        "id": "e8f70f1a",
        "outputId": "6dd68994-c6a5-4f7f-902c-4dda9adea933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "558/558 [==============================] - ETA: 0s - loss: 0.1030 - lwlrap: 0.1286"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Magdalena\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric LWLRAP implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "558/558 [==============================] - 131s 229ms/step - loss: 0.1030 - lwlrap: 0.1286 - val_loss: 0.0718 - val_lwlrap: 0.2145\n",
            "Epoch 2/15\n",
            "558/558 [==============================] - 127s 228ms/step - loss: 0.0708 - lwlrap: 0.2125 - val_loss: 0.0666 - val_lwlrap: 0.2595\n",
            "Epoch 3/15\n",
            "558/558 [==============================] - 126s 226ms/step - loss: 0.0672 - lwlrap: 0.2596 - val_loss: 0.0639 - val_lwlrap: 0.3100\n",
            "Epoch 4/15\n",
            "558/558 [==============================] - 128s 228ms/step - loss: 0.0648 - lwlrap: 0.2952 - val_loss: 0.0634 - val_lwlrap: 0.3182\n",
            "Epoch 5/15\n",
            "558/558 [==============================] - 130s 233ms/step - loss: 0.0631 - lwlrap: 0.3224 - val_loss: 0.0622 - val_lwlrap: 0.3383\n",
            "Epoch 6/15\n",
            "558/558 [==============================] - 129s 231ms/step - loss: 0.0618 - lwlrap: 0.3428 - val_loss: 0.0618 - val_lwlrap: 0.3391\n",
            "Epoch 7/15\n",
            "558/558 [==============================] - 129s 230ms/step - loss: 0.0608 - lwlrap: 0.3570 - val_loss: 0.0618 - val_lwlrap: 0.3418\n",
            "Epoch 8/15\n",
            "558/558 [==============================] - 128s 230ms/step - loss: 0.0600 - lwlrap: 0.3676 - val_loss: 0.0608 - val_lwlrap: 0.3586\n",
            "Epoch 9/15\n",
            "558/558 [==============================] - 130s 234ms/step - loss: 0.0591 - lwlrap: 0.3835 - val_loss: 0.0603 - val_lwlrap: 0.3724\n",
            "Epoch 10/15\n",
            "558/558 [==============================] - 129s 231ms/step - loss: 0.0581 - lwlrap: 0.3950 - val_loss: 0.0605 - val_lwlrap: 0.3647\n",
            "Epoch 11/15\n",
            "558/558 [==============================] - 131s 235ms/step - loss: 0.0574 - lwlrap: 0.4079 - val_loss: 0.0602 - val_lwlrap: 0.3729\n",
            "Epoch 12/15\n",
            "558/558 [==============================] - 130s 233ms/step - loss: 0.0564 - lwlrap: 0.4197 - val_loss: 0.0600 - val_lwlrap: 0.3798\n",
            "Epoch 13/15\n",
            "558/558 [==============================] - 129s 232ms/step - loss: 0.0555 - lwlrap: 0.4324 - val_loss: 0.0602 - val_lwlrap: 0.3821\n",
            "Epoch 14/15\n",
            "558/558 [==============================] - 127s 228ms/step - loss: 0.0545 - lwlrap: 0.4482 - val_loss: 0.0600 - val_lwlrap: 0.3876\n",
            "Epoch 15/15\n",
            "558/558 [==============================] - 130s 232ms/step - loss: 0.0535 - lwlrap: 0.4624 - val_loss: 0.0606 - val_lwlrap: 0.3829\n"
          ]
        }
      ],
      "source": [
        "model_res = make_resnet50_model_cabeza(capas_entrenables=94)\n",
        "model_res.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss='binary_crossentropy',metrics=[LWLRAP(num_classes=80)])\n",
        "model_res.fit(ds_train_ruidosos_resnet, validation_data=ds_val_ruidosos_resnet, epochs=15) #  57 minutos\n",
        "model_res.save('resnet_primer_entrenamiento.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbdc094f",
      "metadata": {
        "id": "cbdc094f"
      },
      "outputs": [],
      "source": [
        "model_resnet_cargado = keras.models.load_model('resnet_primer_entrenamiento.keras', custom_objects={'LWLRAP': LWLRAP})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6ce0eb",
      "metadata": {
        "id": "8e6ce0eb",
        "outputId": "37f2592e-90da-43a1-a826-3e444de13982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0635 - lwlrap: 0.3321"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Magdalena\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric LWLRAP implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_lwlrap improved from -inf to 0.42025, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 40s 132ms/step - loss: 0.0635 - lwlrap: 0.3321 - val_loss: 0.0542 - val_lwlrap: 0.4202\n",
            "Epoch 2/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0554 - lwlrap: 0.4129\n",
            "Epoch 2: val_lwlrap improved from 0.42025 to 0.46011, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 131ms/step - loss: 0.0554 - lwlrap: 0.4129 - val_loss: 0.0505 - val_lwlrap: 0.4601\n",
            "Epoch 3/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0515 - lwlrap: 0.4659\n",
            "Epoch 3: val_lwlrap improved from 0.46011 to 0.48679, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 38s 135ms/step - loss: 0.0515 - lwlrap: 0.4659 - val_loss: 0.0487 - val_lwlrap: 0.4868\n",
            "Epoch 4/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0490 - lwlrap: 0.5022\n",
            "Epoch 4: val_lwlrap improved from 0.48679 to 0.51952, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 39s 140ms/step - loss: 0.0490 - lwlrap: 0.5022 - val_loss: 0.0466 - val_lwlrap: 0.5195\n",
            "Epoch 5/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0464 - lwlrap: 0.5385\n",
            "Epoch 5: val_lwlrap improved from 0.51952 to 0.53720, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 130ms/step - loss: 0.0464 - lwlrap: 0.5385 - val_loss: 0.0455 - val_lwlrap: 0.5372\n",
            "Epoch 6/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0440 - lwlrap: 0.5705\n",
            "Epoch 6: val_lwlrap improved from 0.53720 to 0.54498, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 128ms/step - loss: 0.0440 - lwlrap: 0.5705 - val_loss: 0.0446 - val_lwlrap: 0.5450\n",
            "Epoch 7/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0418 - lwlrap: 0.6029\n",
            "Epoch 7: val_lwlrap improved from 0.54498 to 0.55923, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 128ms/step - loss: 0.0418 - lwlrap: 0.6029 - val_loss: 0.0440 - val_lwlrap: 0.5592\n",
            "Epoch 8/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0394 - lwlrap: 0.6333\n",
            "Epoch 8: val_lwlrap did not improve from 0.55923\n",
            "279/279 [==============================] - 36s 127ms/step - loss: 0.0394 - lwlrap: 0.6333 - val_loss: 0.0444 - val_lwlrap: 0.5520\n",
            "Epoch 9/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0375 - lwlrap: 0.6583\n",
            "Epoch 9: val_lwlrap improved from 0.55923 to 0.56408, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 128ms/step - loss: 0.0375 - lwlrap: 0.6583 - val_loss: 0.0438 - val_lwlrap: 0.5641\n",
            "Epoch 10/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0352 - lwlrap: 0.6872\n",
            "Epoch 10: val_lwlrap improved from 0.56408 to 0.58144, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 36s 130ms/step - loss: 0.0352 - lwlrap: 0.6872 - val_loss: 0.0436 - val_lwlrap: 0.5814\n",
            "Epoch 11/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0335 - lwlrap: 0.7061\n",
            "Epoch 11: val_lwlrap improved from 0.58144 to 0.58225, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 38s 135ms/step - loss: 0.0335 - lwlrap: 0.7061 - val_loss: 0.0447 - val_lwlrap: 0.5823\n",
            "Epoch 12/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0318 - lwlrap: 0.7281\n",
            "Epoch 12: val_lwlrap improved from 0.58225 to 0.58533, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 37s 133ms/step - loss: 0.0318 - lwlrap: 0.7281 - val_loss: 0.0442 - val_lwlrap: 0.5853\n",
            "Epoch 13/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0300 - lwlrap: 0.7500\n",
            "Epoch 13: val_lwlrap improved from 0.58533 to 0.58858, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 37s 132ms/step - loss: 0.0300 - lwlrap: 0.7500 - val_loss: 0.0446 - val_lwlrap: 0.5886\n",
            "Epoch 14/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0282 - lwlrap: 0.7711\n",
            "Epoch 14: val_lwlrap improved from 0.58858 to 0.58934, saving model to resnet_completo.keras\n",
            "279/279 [==============================] - 37s 132ms/step - loss: 0.0282 - lwlrap: 0.7711 - val_loss: 0.0453 - val_lwlrap: 0.5893\n",
            "Epoch 15/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0265 - lwlrap: 0.7902\n",
            "Epoch 15: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 37s 131ms/step - loss: 0.0265 - lwlrap: 0.7902 - val_loss: 0.0465 - val_lwlrap: 0.5760\n",
            "Epoch 16/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0256 - lwlrap: 0.7982\n",
            "Epoch 16: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 37s 131ms/step - loss: 0.0256 - lwlrap: 0.7982 - val_loss: 0.0468 - val_lwlrap: 0.5877\n",
            "Epoch 17/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0245 - lwlrap: 0.8111\n",
            "Epoch 17: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 37s 131ms/step - loss: 0.0245 - lwlrap: 0.8111 - val_loss: 0.0471 - val_lwlrap: 0.5717\n",
            "Epoch 18/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0232 - lwlrap: 0.8262\n",
            "Epoch 18: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 36s 129ms/step - loss: 0.0232 - lwlrap: 0.8262 - val_loss: 0.0480 - val_lwlrap: 0.5789\n",
            "Epoch 19/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0224 - lwlrap: 0.8323\n",
            "Epoch 19: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 37s 132ms/step - loss: 0.0224 - lwlrap: 0.8323 - val_loss: 0.0502 - val_lwlrap: 0.5800\n",
            "Epoch 20/20\n",
            "279/279 [==============================] - ETA: 0s - loss: 0.0215 - lwlrap: 0.8417\n",
            "Epoch 20: val_lwlrap did not improve from 0.58934\n",
            "279/279 [==============================] - 37s 131ms/step - loss: 0.0215 - lwlrap: 0.8417 - val_loss: 0.0509 - val_lwlrap: 0.5677\n"
          ]
        }
      ],
      "source": [
        "# no dejo nada de resnet entrenable\n",
        "for layer in model_resnet_cargado.get_layer(\"resnet50\").layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# por si las dudas activo el resto\n",
        "for layer in model_resnet_cargado.layers:\n",
        "    if not layer.name.startswith(\"resnet50\"):\n",
        "        layer.trainable = True\n",
        "\n",
        "model_resnet_cargado.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),loss='binary_crossentropy',metrics=[LWLRAP(num_classes=80)])\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='resnet_completo.keras',\n",
        "    monitor='val_lwlrap',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")]\n",
        "\n",
        "history_res = model_resnet_cargado.fit(ds_train_curados_resnet, validation_data=ds_val_curados_resnet, epochs=20,callbacks=callbacks) #  12 minutos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d1cada",
      "metadata": {
        "id": "14d1cada"
      },
      "outputs": [],
      "source": [
        "best_model_resnet = keras.models.load_model('resnet_completo.keras', custom_objects={'LWLRAP': LWLRAP})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21959fa9",
      "metadata": {
        "id": "21959fa9",
        "outputId": "45af7e91-0e85-4730-e4fd-5ab5b35b1420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141/141 [==============================] - 14s 96ms/step\n"
          ]
        }
      ],
      "source": [
        "df_submission_resnet = generar_submission_resnet(best_model_resnet, df_test, path_salida='submission_resnet.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21f93cd",
      "metadata": {
        "id": "e21f93cd"
      },
      "source": [
        "## Mobilnet entrenado completamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3d5fa1-c3e6-4ca0-b0da-2036d11ce00e",
      "metadata": {
        "id": "ee3d5fa1-c3e6-4ca0-b0da-2036d11ce00e"
      },
      "outputs": [],
      "source": [
        "ds_train_ruidosos_M ,ds_val_ruidosos_M = crear_datasets_train_val(df_train_ruidosos,carpeta_audio='train_noisy_sin_silencios',prep='mobile',batch_size=16,ratio=0.2,shuffle=True)\n",
        "\n",
        "ds_train_curados_M,ds_val_curados_M = crear_datasets_train_val(df_train_curados,carpeta_audio='train_curated_sin_silencios',prep='mobile',batch_size=16,ratio=0.2,shuffle=True)\n",
        "\n",
        "ds_train_curados_M_aumentado, ds_val_curados_M_aumentado = crear_datasets_train_val_aumentado_strat(df_train_curados,carpeta_audio='train_curated_sin_silencios',prep='mobile',batch_size=16,ratio=0.2)\n",
        "\n",
        "\n",
        "\n",
        "ds_train_aumentado_M = ds_train_ruidosos_M.concatenate(ds_val_ruidosos_M)\n",
        "ds_train_aumentado_M=ds_train_aumentado_M.concatenate(ds_train_curados_M)\n",
        "ds_train_aumentado_M=ds_train_aumentado_M.concatenate(ds_train_curados_M_aumentado)\n",
        "\n",
        "\n",
        "ds_val_aumentado_M = ds_val_curados_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddcec382-f756-428f-968e-9d342344f9e9",
      "metadata": {
        "id": "ddcec382-f756-428f-968e-9d342344f9e9",
        "outputId": "a280b21c-0361-4e10-e4b7-1f0835b11bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:From C:\\Users\\ureca\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "1984/1984 [==============================] - ETA: 0s - loss: 0.0966 - lwlrap: 0.1422"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ureca\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric LWLRAP implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1984/1984 [==============================] - 275s 133ms/step - loss: 0.0966 - lwlrap: 0.1422 - val_loss: 0.0594 - val_lwlrap: 0.3730\n",
            "Epoch 2/20\n",
            "1984/1984 [==============================] - 249s 125ms/step - loss: 0.0675 - lwlrap: 0.2596 - val_loss: 0.0497 - val_lwlrap: 0.5029\n",
            "Epoch 3/20\n",
            "1984/1984 [==============================] - 249s 125ms/step - loss: 0.0624 - lwlrap: 0.3281 - val_loss: 0.0430 - val_lwlrap: 0.5805\n",
            "Epoch 4/20\n",
            "1984/1984 [==============================] - 251s 126ms/step - loss: 0.0587 - lwlrap: 0.3804 - val_loss: 0.0398 - val_lwlrap: 0.6211\n",
            "Epoch 5/20\n",
            "1984/1984 [==============================] - 250s 126ms/step - loss: 0.0559 - lwlrap: 0.4184 - val_loss: 0.0360 - val_lwlrap: 0.6644\n",
            "Epoch 6/20\n",
            "1984/1984 [==============================] - 250s 126ms/step - loss: 0.0535 - lwlrap: 0.4488 - val_loss: 0.0332 - val_lwlrap: 0.6921\n",
            "Epoch 7/20\n",
            "1984/1984 [==============================] - 249s 126ms/step - loss: 0.0510 - lwlrap: 0.4828 - val_loss: 0.0325 - val_lwlrap: 0.7034\n",
            "Epoch 8/20\n",
            "1984/1984 [==============================] - 249s 126ms/step - loss: 0.0490 - lwlrap: 0.5070 - val_loss: 0.0288 - val_lwlrap: 0.7501\n",
            "Epoch 9/20\n",
            "1984/1984 [==============================] - 247s 125ms/step - loss: 0.0472 - lwlrap: 0.5290 - val_loss: 0.0278 - val_lwlrap: 0.7535\n",
            "Epoch 10/20\n",
            "1984/1984 [==============================] - 247s 124ms/step - loss: 0.0455 - lwlrap: 0.5501 - val_loss: 0.0258 - val_lwlrap: 0.7747\n",
            "Epoch 11/20\n",
            "1984/1984 [==============================] - 248s 125ms/step - loss: 0.0443 - lwlrap: 0.5655 - val_loss: 0.0249 - val_lwlrap: 0.7946\n",
            "Epoch 12/20\n",
            "1984/1984 [==============================] - 255s 128ms/step - loss: 0.0428 - lwlrap: 0.5815 - val_loss: 0.0247 - val_lwlrap: 0.7954\n",
            "Epoch 13/20\n",
            "1984/1984 [==============================] - 250s 126ms/step - loss: 0.0416 - lwlrap: 0.5952 - val_loss: 0.0243 - val_lwlrap: 0.7995\n",
            "Epoch 14/20\n",
            "1984/1984 [==============================] - 250s 126ms/step - loss: 0.0407 - lwlrap: 0.6067 - val_loss: 0.0237 - val_lwlrap: 0.8040\n",
            "Epoch 15/20\n",
            "1984/1984 [==============================] - 252s 127ms/step - loss: 0.0397 - lwlrap: 0.6187 - val_loss: 0.0245 - val_lwlrap: 0.8035\n",
            "Epoch 16/20\n",
            "1984/1984 [==============================] - 251s 126ms/step - loss: 0.0389 - lwlrap: 0.6275 - val_loss: 0.0227 - val_lwlrap: 0.8163\n",
            "Epoch 17/20\n",
            "1984/1984 [==============================] - 253s 127ms/step - loss: 0.0378 - lwlrap: 0.6400 - val_loss: 0.0239 - val_lwlrap: 0.8113\n",
            "Epoch 18/20\n",
            "1984/1984 [==============================] - 259s 130ms/step - loss: 0.0372 - lwlrap: 0.6471 - val_loss: 0.0238 - val_lwlrap: 0.8146\n",
            "Epoch 19/20\n",
            "1984/1984 [==============================] - 255s 129ms/step - loss: 0.0370 - lwlrap: 0.6472 - val_loss: 0.0236 - val_lwlrap: 0.8215\n",
            "Epoch 20/20\n",
            "1984/1984 [==============================] - 269s 136ms/step - loss: 0.0361 - lwlrap: 0.6587 - val_loss: 0.0225 - val_lwlrap: 0.8244\n"
          ]
        }
      ],
      "source": [
        "model_mobile = make_mobilenet_model_cabeza(capas_entrenables=156, cabeza_basica = False)\n",
        "model_mobile.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),loss='binary_crossentropy',metrics=[LWLRAP(num_classes=80)])\n",
        "\n",
        "model_mobile.fit(ds_train_aumentado_M, validation_data=ds_val_curados_M, epochs=20)\n",
        "model_mobile.save('mobilenet_entrenamiento.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2798a56c-2c9b-4417-8fc9-d8fc79c95aad",
      "metadata": {
        "id": "2798a56c-2c9b-4417-8fc9-d8fc79c95aad"
      },
      "outputs": [],
      "source": [
        "best_model_mobile = keras.models.load_model('mobilenet_entrenamiento.keras', custom_objects={'LWLRAP': LWLRAP})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c47caa4-dff6-4125-98c8-f4e35b3a21ec",
      "metadata": {
        "id": "7c47caa4-dff6-4125-98c8-f4e35b3a21ec",
        "outputId": "00ad2442-c7fc-41a3-b3fb-33a1915764c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141/141 [==============================] - 16s 111ms/step\n"
          ]
        }
      ],
      "source": [
        "df_submission_mobile = generar_submission_mobile(best_model_mobile, df_test, path_salida='submission_mobile.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d0e01c-c29d-4133-9ce7-08d8c9d8aff5",
      "metadata": {
        "id": "41d0e01c-c29d-4133-9ce7-08d8c9d8aff5",
        "outputId": "2dfc722a-ae70-4ff1-a4d9-5b3a617166d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4481\n"
          ]
        }
      ],
      "source": [
        "print(len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f5af9a-a28c-4604-990e-b51cff6fd431",
      "metadata": {
        "id": "73f5af9a-a28c-4604-990e-b51cff6fd431"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "csv_mobile = pd.read_csv(\"submission_mobile.csv\")\n",
        "csv_resnet = pd.read_csv(\"submission_resnet.csv\")\n",
        "\n",
        "\n",
        "\n",
        "columnas_numericas = csv_mobile.columns.drop([\"fname\"])\n",
        "csv_mobile[columnas_numericas] = csv_mobile[columnas_numericas].astype(float)\n",
        "csv_resnet[columnas_numericas] = csv_resnet[columnas_numericas].astype(float)\n",
        "\n",
        "\n",
        "promedio = (csv_mobile[columnas_numericas] + csv_resnet[columnas_numericas]) / 2\n",
        "\n",
        "\n",
        "promedio.insert(0, \"fname\", csv_mobile[\"fname\"])\n",
        "promedio.to_csv(\"submission_promedio_mobile_resnet.csv\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELO 3 (MixUp)"
      ],
      "metadata": {
        "id": "H_xOU3AJ5G80"
      },
      "id": "H_xOU3AJ5G80"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lznqkirw7J8S"
      },
      "id": "Lznqkirw7J8S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c033514d-b642-479b-acd3-a96aab6e4fa6",
      "metadata": {
        "id": "c033514d-b642-479b-acd3-a96aab6e4fa6"
      },
      "outputs": [],
      "source": [
        "input_shape=(224, 224, 3)\n",
        "\n",
        "\n",
        "filepath('modelo3.keras') #SE PUEDE MODIFICAR\n",
        "\n",
        "\n",
        "dataset_noisy_train, dataset_noisy_val = crear_datasets_train_val(df_train_ruidosos,carpeta_audio='train_ruidosos_sin_silencios_resample',prep='modelo3',batch_size=32,ratio=0.1,shuffle=True)\n",
        "dataset_curated_train, dataset_curated_val = crear_datasets_train_val_aumentado_strat(df_train_curados,carpeta_audio='train_curated_sin_silencios_resample',prep='modelo3',batch_size=32,ratio=0.1)\n",
        "\n",
        "\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "base_model = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False)\n",
        "x = base_model(inputs)\n",
        "avg = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "dense1 = tf.keras.layers.Dense(512, activation=\"relu\")(avg)\n",
        "dropout = tf.keras.layers.Dropout(0.3)(dense1)\n",
        "dense2 = tf.keras.layers.Dense(256, activation=\"relu\")(dropout)\n",
        "dropout2 = tf.keras.layers.Dropout(0.3)(dense2)\n",
        "output = tf.keras.layers.Dense(80, activation=\"sigmoid\")(dropout2)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    if \"conv4_block\" in layer.name or \"conv5_block\" in layer.name:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "metrics=[LWLRAP(num_classes=80)])\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=filepath,\n",
        "    monitor='val_lwlrap',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_noisy_train,\n",
        "    validation_data=dataset_noisy_val,\n",
        "    epochs=20,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    if \"conv3_block\" in layer.name:\n",
        "        layer.trainable = True\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=lr/5),\n",
        "metrics=[LWLRAP(num_classes=80)])\n",
        "\n",
        "history2 = model.fit(\n",
        "    dataset_noisy_train,\n",
        "    validation_data=dataset_noisy_val,\n",
        "    epochs=5,\n",
        ")\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(),\n",
        "metrics=[LWLRAP(num_classes=80)])\n",
        "\n",
        "\n",
        "\n",
        "history3 = model.fit(\n",
        "    dataset_curated_train,\n",
        "    validation_data=dataset_curated_val,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint_cb]\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}