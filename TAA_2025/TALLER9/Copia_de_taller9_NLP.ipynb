{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjsf78ymkS0E"
      },
      "source": [
        "#  <center> Taller  de Aprendizaje Automático </center>\n",
        "##  <center> Taller 9: *Natural Language Processing* (NLP)  </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUfgw7Siq3td"
      },
      "source": [
        "## Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4y3HbLOkOVD"
      },
      "source": [
        "La siguiente actividad propone el abordaje de un problema de procesamiento de lenguaje natural (NLP) utilizando herramientas de *embedding* y modelos RNN. El conjunto de datos que se utilizará es IMDb, el cual corresponde a un problema de clasificación donde se tienen 50000 criticas de películas (35000 de *train* y 15000 de *test*), y se quiere estimar si éstas son críticas positivas (1) o negativas (0).\n",
        "\n",
        "La propuesta consiste en entender y reproducir los pasos de la sección *Sentiment Analysis* para los datos **sin procesar**, agregando algunas variantes como mitigar el sobreajuste y entender la herramienta *embeddings*.\n",
        "\n",
        "En este Taller también se introduce la biblioteca *Streamlit*, utilizada para desarrollar prototipos de aplicaciones web de aprendizaje automático. Aquellos que así lo deseen, podrán generar de manera sencilla una aplicación web que clasifique las críticas proporcionadas por los usuarios. Además, se presenta la biblioteca HuggingFace, la cual permite realizar inferencias con modelos preentrenados y realizar fine-tuning para adaptarlos a esta tarea específica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY6vz2Ekj8ig"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "\n",
        "*   Aplicar modelos basados en RNN a un problema de NLP.\n",
        "*   Trabajar con embeddings para secuencias de texto, en particular embeddings preentrenados.\n",
        "*   Utilizar herramientas para la visualización de embeddings.\n",
        "*  (Opcional, no evaluado) Desarrollar una aplicación web que clasifique críticas proporcionadas por los usarios\n",
        "*  (Opcional, no evaluado) Utilizar la biblioteca *HuggingFace* para utilizar modelos preentrenados y realizar fine-tunning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "common-destiny"
      },
      "source": [
        "## Formas de trabajo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVgxoLgl1-KA"
      },
      "source": [
        "### Opción 1: Trabajar localmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-gallery"
      },
      "source": [
        "Descargar los datos en su máquina personal y trabajar en su propio ambiente de desarrollo.\n",
        "\n",
        "`conda activate TAA-py310`              \n",
        "`jupyter-notebook`    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfcTy55R2A7w"
      },
      "source": [
        "Los paquetes faltantes se pueden instalar desde el notebook haciendo:     \n",
        "` !pip install paquete_faltante`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-sport"
      },
      "source": [
        "### Opción 2:  Trabajar en *Colab*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lined-candle"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/TAA-fing/TAA-2025/blob/main/talleres/taller9_NLP.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Ejecutar en Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-jewel"
      },
      "source": [
        "Se puede trabajar en Google Colab. Para ello es necesario contar con una cuenta de **google drive** y ejecutar un notebook almacenado en dicha cuenta. De lo contrario, no se conservarán los cambios realizados en la sesión. En caso de ya contar con una cuenta, se puede abrir el notebook y luego ir a `Archivo-->Guardar una copia en drive`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8OOpzq91ITq"
      },
      "source": [
        "La siguiente celda realiza la configuración necesaria para obtener datos desde la plataforma Kaggle. Le solicitará que suba el archivo *kaggle.json* asociado a su cuenta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "LlKwJ6sKmhBt",
        "outputId": "9f2b9b78-3746-4f01-9d99-c103ea75c349"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae6eeba7-469b-4bd7-bb57-400bf82ee58e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae6eeba7-469b-4bd7-bb57-400bf82ee58e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 64 bytes\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import files\n",
        "\n",
        "# El siguiente archivo solicitado es para habilitar la API de Kaggle en el entorno que está trabajando.\n",
        "# Este archivo se descarga entrando a su perfíl de Kaggle, en la sección API, presionando donde dice: Create New API Token\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "#Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj1R6mm14u0t"
      },
      "source": [
        "# Parte 1: Análisis y preprocesamiento de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M44JXP4Z41BI"
      },
      "source": [
        "Se utilizará el conjunto de IMDb provisto por Kaggle. Se tienen 50000 criticas de películas que al igual que en el *Taller 2* se utilizarán 35000 para *train* y 15000 para *test*.\n",
        "\n",
        "*   Ejecutar la siguiente celda para descargar el conjunto y verificar que los conjuntos tienen la cantidad de instancias esperadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwAZeeqamXo2",
        "outputId": "7bf2d0eb-16d9-4c6a-a205-23ac076740c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            "  0% 0.00/25.7M [00:00<?, ?B/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 1.22GB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descarga la base IMDb de Kaggle\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj1ZMjmtn3cX",
        "outputId": "e11f4a4d-fa4c-43c6-e603-a723afd360dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Size: 35000 Test Size: 15000\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Se descomprime el archivo descargado\n",
        "with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')\n",
        "\n",
        "# Se levanta como pandas DataFrame\n",
        "data_file = 'IMDB Dataset.csv'\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "#Separación de Conjuntos\n",
        "N=35000\n",
        "X_train = data.loc[:N-1, 'review'].values\n",
        "y_train = data.loc[:N-1, 'sentiment'].values == 'positive'\n",
        "X_test = data.loc[N:, 'review'].values\n",
        "y_test = data.loc[N:, 'sentiment'].values == 'positive'\n",
        "\n",
        "# Armado de los Tensorflow Datasets\n",
        "data_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "data_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "#Verificación\n",
        "print('Train Size:', len(data_train),'Test Size:', len(data_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQpeE5zh8ZlC"
      },
      "source": [
        "*   Del conjunto de entrenamiento visualizar tanto una review positiva como una negativa. Se sugiere ir a los Notebooks del *Capítulo 16*. Puede ser útil el uso del método *.skip()*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMp1E9d27ro5",
        "outputId": "5c72e883-2a4f-456c-8572-59c22d1435d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me abo ...\n",
            "Label: True\n",
            "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece ...\n",
            "Label: True\n",
            "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is wi ...\n",
            "Label: True\n",
            "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, J ...\n",
            "Label: False\n"
          ]
        }
      ],
      "source": [
        "for review, label in data_train.take(4):\n",
        "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"Label:\", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYU0vZiD65J1"
      },
      "source": [
        "*   Reservar unas 5000 críticas de los datos de entrenamiento para validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D55BIKVp7M2K",
        "outputId": "bfae3fe0-6efe-41e6-a1d1-856d777d877b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño de train:  30000\n",
            "Tamaño de val:  5000\n"
          ]
        }
      ],
      "source": [
        "train_size = len(data_train)\n",
        "\n",
        "val_size = 5000\n",
        "\n",
        "data_val = data_train.skip(train_size - val_size)\n",
        "data_train = data_train.take(train_size - val_size)\n",
        "\n",
        "print('Tamaño de train: ', len(data_train))\n",
        "print('Tamaño de val: ', len(data_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQ3PHH338ys"
      },
      "source": [
        "* Prepare los batches para todos los conjuntos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIFFRf6e4BmI"
      },
      "outputs": [],
      "source": [
        "train_set =data_train.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = data_val.batch(32).prefetch(1)\n",
        "test_set = data_test.batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEdt3JsjIjIS"
      },
      "source": [
        "*   Keras proporciona una capa de `TextVectorization` para el preprocesamiento básico de texto. Explique el funcionamiento y adapte una capa para los datos de entrenemiento en cuestión. Se sugiere leer la sección asociada a esta capa en el *Capítulo 13* del libro. Utilice un tamaño de vocabulario de 10000 palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhvC46EhHf2K"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMkt4V3_4NGm"
      },
      "source": [
        "* Obtenga el diccionario de la capa `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-LMMrdwH4YP",
        "outputId": "e938054a-6361-4c57-b5f8-8b800e7cf4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: the\n",
            "3: and\n",
            "4: a\n",
            "5: of\n",
            "6: to\n",
            "7: is\n",
            "8: in\n",
            "9: it\n",
            "10: i\n",
            "11: this\n",
            "12: that\n",
            "13: br\n",
            "14: was\n",
            "15: as\n",
            "16: with\n",
            "17: for\n",
            "18: movie\n",
            "19: but\n"
          ]
        }
      ],
      "source": [
        "dictionary = text_vec_layer.get_vocabulary()\n",
        "for numero, valor in enumerate(dictionary[:20]):\n",
        "  print(f'{numero}: {valor}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL0ayaj4C90z"
      },
      "source": [
        "* Siguiendo el ejemplo adjunto en la [documentación](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) de la capa `TextVectorization`. Cree un modelo de keras que cuente únicamente con esta capa y observe el resultado de pasar una crítica cualquiera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSzYSkpPC9sr",
        "outputId": "7cf0cc97-a9b5-48a9-c78c-5f27a91cd90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n"
          ]
        }
      ],
      "source": [
        "text_vec_model = tf.keras.Sequential([\n",
        "    text_vec_layer\n",
        "    ])\n",
        "\n",
        "\n",
        "pred = text_vec_model.predict(train_set.take(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2HMib8Q2zEt",
        "outputId": "ee2d9c12-33e9-4320-b74e-724d18de0e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto original:\n",
            "**Warning! Mild Spoilers Ahead!**<br /><br />(Yes, I realize it's tough to spoil an historical docum ...\n",
            "\n",
            "Texto vectorizado:\n",
            "[1727 3616 1200    1   13  417   10  912   30 1263    6 2305   34 1269\n",
            "  673   19   10   79 2460   47    5    2 7197    3    1   13   11    7\n",
            "   34 3068  673   22   41   80    5    2 1832  895   19   82  660    6\n",
            "    2   66  513    9   80    2    1  112   22  289   46    6  371    2\n",
            "   66    5 4193   19  242   12    5    4 8745    1    2 7072 1302    3\n",
            "    2  728 1821   16   95   24   53  146    3  984   71   35   57   27\n",
            "    8    4 1283    1   13   13    8    4 1485  283 4193    7 8872   44\n",
            "    2  710   68   34  761  232   35   57   27  905   15    2  102   24\n",
            " 3002   91 1340    4    1    6   61   35  208 4444   11    7   32    2\n",
            "   53  495 1052    2 1463    3 1368    5    2 2727  895   12    2  677\n",
            "   69    6    1  142    6  407   11   66  290   13   13    2   89 4579\n",
            "    5    2   20 1525    4 3197    5  121  928    4  965 1602 3858   86\n",
            "    4 8745    1  307   81    4  861    5 6750   12  172  611    7  176\n",
            "   50    3   69    2  673   76 1509    6  506   30 1380  257    9  231\n",
            "   57   26   76 1178    2 1047  945    6 6385 2008    2 1578    5 1545\n",
            "    4 4450    1   13   13   91    5  257   32  544 1911 1801    2 3716\n",
            "  988    2    1 2912    7 6748  108   15  916  102   12   74   26 1888\n",
            "    6  118   24 7126   81 3606 1129  305   22   64    2 2912   19   82\n",
            "    2    1 3611    7    4   52  734  419    5 1438  382 1368   13   13\n",
            "    2 6660    8   61    1   24 1932    6   27  430    3  249    7  382\n",
            "  470   31   30 4580  121    3  330 2959    8    2 2792 1002  105   93\n",
            "    2  515   22   64  150  118   36   78  432    3  791   19 1917 2171\n",
            "   43   95   13   13    2   64 1566  149   10   26    6  131   43   11\n",
            "    7   12    2  593 5872  940   10   39    1   68    1 1830    3  151\n",
            " 4399    2   66   31   32   35   68  231 1192   41    6 5341   53  695\n",
            "    1   13 1264  358    2  116  673  193  120  108    1 5083    5 2727\n",
            "  382 1368    3  470 6162   46    5  302    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ],
      "source": [
        "for reviews_batch, _ in train_set.take(1):\n",
        "    review_text = reviews_batch[0].numpy().decode(\"utf-8\")\n",
        "    print(\"Texto original:\")\n",
        "    print(review_text[:100], \"...\")\n",
        "    vectorized_batch = text_vec_layer(reviews_batch)\n",
        "    print(\"\\nTexto vectorizado:\")\n",
        "    print(vectorized_batch[0].numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUSqc8-o4Xj2"
      },
      "source": [
        "*   Utilizando esta capa ¿cuáles son los largos de secuencias para los primeros *batches*? ¿Es un problema a resolver que todos tengan largos distintos? ¿Por qué?."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP2mCScC4Zz4",
        "outputId": "98d4f9ca-cfa8-4077-8830-f5885d297856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forma del batch vectorizado: (32, 960)\n",
            "Batch vectorizado: tf.Tensor(\n",
            "[[  44   23 1095  697   40   67  263   58   16 2125   23   78   63  352\n",
            "    11   18   44   21    2   77  587   23   41  181    6   65 3095   87\n",
            "   303 2086   11    7    4  208   65   22   34 9198    5    1    8 1737\n",
            "     1  754 1779  122  527   46 7675    1  618    7    2    1    5 3404\n",
            "     3  264   10   69  120    1   43   42  172    7   55    8  603   55\n",
            "     2  190   12   59   14 2814   16  299 7403  112   22 6418   17   42\n",
            "   892 7675    7    4  718   44  120   48   14   29    1    1    3    1\n",
            "     1   24    1  452 1201    9    7    4  177   50   66  425  113 6666\n",
            "    24 6930  290    8    4  427    5 1563 8068    5 9712  433    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]], shape=(1, 960), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for reviews_batch, _ in train_set.take(1):\n",
        "    vectorized = text_vec_layer(reviews_batch)\n",
        "    print(\"Forma del batch vectorizado:\", vectorized.shape)\n",
        "    print(\"Batch vectorizado:\", vectorized[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdeBos9x3r3L",
        "outputId": "f894b805-d603-49fd-8418-b4967ca04e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Assignment is an outstanding thriller with several plot twists driven by character, rather than star turns, the need to stage special effects, obligatory romance, and endless car chases. However,  ...\n"
          ]
        }
      ],
      "source": [
        "for reviews_batch, labels_batch in train_set.take(1):\n",
        "    for review in reviews_batch[:1]:\n",
        "        print(review.numpy().decode(\"utf-8\")[:200], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwHB8XeLV-33"
      },
      "source": [
        "Las capas esperan entradas de tama;o fijo por batch\n",
        "Durante el .adapt(...), si no se define explícitamente output_sequence_length, la capa estima una longitud adecuada con base en la distribución de longitudes del texto del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU1ENOlD249d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZlLb6v4OHS"
      },
      "source": [
        "# Parte 2: *Embedding*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZeKwMu4dlD"
      },
      "source": [
        "* Cree y entrene el modelo que aparece al final de la sección *Sentiment Analysis* y previo a la sección *Masking* del *Capítulo 16*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1zMpB7iC9sr",
        "outputId": "d67822b2-d940-4569-e0fe-78475e456b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.5091 - loss: 0.6936 - val_accuracy: 0.4960 - val_loss: 0.6925\n",
            "Epoch 2/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 39ms/step - accuracy: 0.5123 - loss: 0.6916 - val_accuracy: 0.4958 - val_loss: 0.6935\n"
          ]
        }
      ],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYunxz_uRruE"
      },
      "source": [
        "* Implemente el uso de *Masking* en el modelo que ya utilizó. ¿Por qué podría ser esto útil para el aprendizaje?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24pjFwc0C9sr",
        "outputId": "0def0188-869f-41f2-cd61-ba5fc6998d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 40ms/step - accuracy: 0.7001 - loss: 0.5538 - val_accuracy: 0.8406 - val_loss: 0.3842\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 36ms/step - accuracy: 0.8798 - loss: 0.2990 - val_accuracy: 0.8812 - val_loss: 0.2899\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9330 - loss: 0.1782 - val_accuracy: 0.9008 - val_loss: 0.2539\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9641 - loss: 0.1070 - val_accuracy: 0.8850 - val_loss: 0.3351\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9776 - loss: 0.0684 - val_accuracy: 0.8838 - val_loss: 0.3855\n"
          ]
        }
      ],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0ynw4zy1w5"
      },
      "source": [
        "* Observe que el modelo se sobreajusta a los datos. Utilice alguna técnica vista para regularizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ShtxhoHWnrO8",
        "outputId": "6f063fe7-1ba8-4567-8e70-be56e45b82cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,137,606</span> (15.78 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,137,606\u001b[0m (15.78 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,379,201</span> (5.26 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,379,201\u001b[0m (5.26 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,758,405</span> (10.52 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,758,405\u001b[0m (10.52 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InJVu0DpC9ss"
      },
      "outputs": [],
      "source": [
        "model_dropout = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128, dropout=0.3),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_dropout.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxjmIiroXePR",
        "outputId": "712df1ec-e7ac-44f7-86f0-650682ec83f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42ms/step - accuracy: 0.6907 - loss: 0.5569 - val_accuracy: 0.8768 - val_loss: 0.3110\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 49ms/step - accuracy: 0.9040 - loss: 0.2426 - val_accuracy: 0.8948 - val_loss: 0.2637\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 49ms/step - accuracy: 0.9417 - loss: 0.1607 - val_accuracy: 0.8880 - val_loss: 0.2954\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 56ms/step - accuracy: 0.9621 - loss: 0.1067 - val_accuracy: 0.8706 - val_loss: 0.4106\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.9757 - loss: 0.0737 - val_accuracy: 0.8822 - val_loss: 0.4093\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9841 - loss: 0.0486 - val_accuracy: 0.8752 - val_loss: 0.4931\n"
          ]
        }
      ],
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=4,  # esperar 4 épocas sin mejora\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model_dropout.fit(\n",
        "    train_set,\n",
        "    validation_data=valid_set,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am-ML9T_n6dy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "model_dropout3 = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.LSTM(128, dropout=0.3,return_sequences=True,kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    tf.keras.layers.LSTM(128, dropout=0.5,kernel_regularizer=regularizers.l2(0.00001)),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_dropout.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model_dropout3.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJQhMx-foA6P",
        "outputId": "0aa923fe-f5d1-4d6c-e0c3-70d329939cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 63ms/step - accuracy: 0.6950 - loss: 0.5668 - val_accuracy: 0.8512 - val_loss: 0.3512\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 63ms/step - accuracy: 0.8777 - loss: 0.3095 - val_accuracy: 0.8582 - val_loss: 0.3339\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 63ms/step - accuracy: 0.8955 - loss: 0.2752 - val_accuracy: 0.8756 - val_loss: 0.3276\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - accuracy: 0.8753 - loss: 0.2926 - val_accuracy: 0.7086 - val_loss: 0.5686\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 65ms/step - accuracy: 0.7451 - loss: 0.4922 - val_accuracy: 0.8558 - val_loss: 0.3750\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 67ms/step - accuracy: 0.9085 - loss: 0.2554 - val_accuracy: 0.8722 - val_loss: 0.3315\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 96ms/step - accuracy: 0.9382 - loss: 0.1874 - val_accuracy: 0.8688 - val_loss: 0.3713\n"
          ]
        }
      ],
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=4,  # esperar 4 épocas sin mejora\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model_dropout3.fit(\n",
        "    train_set,\n",
        "    validation_data=valid_set,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBClMXu1fYa1"
      },
      "outputs": [],
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "X731JF1Wb-42",
        "outputId": "290a955a-830b-4e05-ce7b-3f04529ef886"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'comet_ml'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-1786692548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcomet_ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'B1BQZyV4ItS1xcnh2qC0PDToK'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'comet_ml'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from comet_ml import Experiment\n",
        "API_KEY = 'B1BQZyV4ItS1xcnh2qC0PDToK'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQbGAvMIdKQe"
      },
      "outputs": [],
      "source": [
        "exp = Experiment(api_key=API_KEY,\n",
        "                 project_name='Taller9',\n",
        "                 auto_param_logging=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orjrgW4GdkUA"
      },
      "outputs": [],
      "source": [
        "embeddings = model.layers[1].get_weights()[0]\n",
        "\n",
        "dictionary[0] = \"<pad>\"\n",
        "\n",
        "\n",
        "exp.log_embedding(vectors= embeddings , labels= dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqU7n4gMZc5X"
      },
      "source": [
        "* El modelo cuenta con una capa de entrada de *embedding* la cual abarca la mayoría de los parámetros entrenables. En este caso un *embedding* es un vector entrenable que representa una palabra en nuevo espacio cuyo tamaño es un hiperparámetro. La concatenación de estos vectores conforma la matriz de *embedding*, donde su cantidad de filas corresponde a la suma del tamaño del vocabulario y la cantidad de columnas a las dimensiones de los vectores (*embed_size*). Al igual que una matriz de pesos, ésta se inicializa de forma aleatoria, y actualiza sus valores para cada *step* de entrenamiento.\n",
        "\n",
        "  *   ¿Cuál es la ventaja de utilizar una capa de *embedding*? (Ver la sección *Encoding Categorical Features Using Embeddings* del *Capítulo 13* del libro.)\n",
        "  *   Visualizar una representación del espacio de *embedding* utilizando *Comet*. **Importante:** para evitar errores, modifique del diccionario el valor del espacio por `<pad>`.\n",
        "\n",
        "  Para este último punto se recomienda seguir el siguiente ejemplo: [logging-embeddings](https://www.comet.ml/docs/user-interface/embeddings/#logging-embeddings).\n",
        "\n",
        "*   Para su mejor modelo: visualizar a qué distancias se encuentran las palabras unas de otras tanto en la representación a baja dimensión como en el espacio de *embedding*. Sobre todo probar con adjetivos positivos (*wonderful*, *excellent*, etc.) y negativos (*ugly*, *boring*, etc.) comparando los resultados. ¿Qué logra observar?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfSujuskbZog"
      },
      "source": [
        "# Parte 3: *Embedding Preentrenado*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4y9GduFbiZD"
      },
      "source": [
        "Una de las técnicas para mejorar el desempeño en este tipo de problemas es utilizar *embeddings* ya entrenados.\n",
        "Siguiendo el ejemplo [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACU7xlN8bqzP"
      },
      "source": [
        "*   Descargar el *embedding* preentrenado [GloVE](https://nlp.stanford.edu/projects/glove/) que aparece en la sección *Load pre-trained word embeddings*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R368yCtObhyk",
        "outputId": "eaa5fa81-f816-45a6-b823-b9929a545e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-28 17:12:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-06-28 17:12:54--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.68MB/s    in 2m 42s  \n",
            "\n",
            "2025-06-28 17:15:37 (5.08 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ARmJXcbyhU"
      },
      "source": [
        "* Preparar la nueva matriz de *embedding*. ¿Cuántas palabras del conjunto de entrenamiento se encuentran en el vocabulario de GloVE? ¿Cuántas no?."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev8_K9Skssu7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nfz9Ee1b0yP",
        "outputId": "ca8ce3c8-484f-47bc-8772-9bb659a7e5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "path_to_glove_file = '/content/glove.6B.100d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTfbZ7eoamO-"
      },
      "outputs": [],
      "source": [
        "word_index = dict(zip(dictionary, range(len(dictionary))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kcu9IswaPL4",
        "outputId": "3c9e1d70-b506-43db-818c-7ca7ca3cf52c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 9699 words (301 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(dictionary)\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipkjXoNecR2_"
      },
      "source": [
        "* Entrenar el modelo con la nueva matriz de *embedding* de manera que los valores de ésta se mantengan fijos (ver parámetro en la capa de *embedding*). Utilice masking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7smspfWGC9sw"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    trainable=False,\n",
        "    mask_zero = True\n",
        ")\n",
        "embedding_layer.build((1,))\n",
        "embedding_layer.set_weights([embedding_matrix])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6prBZhvbQi-",
        "outputId": "4a3b78c2-fb0a-4a64-c6b5-c2a0d89bea42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 39ms/step - accuracy: 0.6543 - loss: 0.6034 - val_accuracy: 0.7958 - val_loss: 0.4357\n",
            "Epoch 2/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 36ms/step - accuracy: 0.8263 - loss: 0.3886 - val_accuracy: 0.8372 - val_loss: 0.3592\n"
          ]
        }
      ],
      "source": [
        "model_emb_not_trainable = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.GRU(128, dropout = 0.4),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_emb_not_trainable.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model_emb_not_trainable.fit(train_set, validation_data=valid_set, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwqxdPALEyJ4"
      },
      "source": [
        "* Entrene nuevamente el modelo pero con el *embedding* entrenable. Modifique el *learning rate*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLKSMCJnC9sw",
        "outputId": "43b42f67-7a72-4ac8-e6ce-758b030cc4b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.8511 - loss: 0.3402 - val_accuracy: 0.8498 - val_loss: 0.3386\n",
            "Epoch 2/2\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 36ms/step - accuracy: 0.8626 - loss: 0.3167 - val_accuracy: 0.8594 - val_loss: 0.3247\n"
          ]
        }
      ],
      "source": [
        "embedding_layer_trainable = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    trainable=True,\n",
        "    mask_zero = True\n",
        ")\n",
        "embedding_layer_trainable.build((1,))\n",
        "embedding_layer_trainable.set_weights([embedding_matrix])\n",
        "\n",
        "\n",
        "model_trainable = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    embedding_layer_trainable,\n",
        "    tf.keras.layers.GRU(128, dropout = 0.4),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_trainable.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model_emb_not_trainable.fit(train_set, validation_data=valid_set, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "KYnq8NRieTcy",
        "outputId": "3cbefc78-e001-4483-c768-d07fbe61f46c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m99,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,137,606</span> (15.78 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,137,606\u001b[0m (15.78 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,379,201</span> (5.26 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,379,201\u001b[0m (5.26 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,758,405</span> (10.52 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,758,405\u001b[0m (10.52 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2oDqM-zieXCq",
        "outputId": "510b7395-cafb-4dc1-f345-35f41c76e521"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">88,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m88,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,265,350</span> (4.83 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,265,350\u001b[0m (4.83 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,449</span> (345.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m88,449\u001b[0m (345.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">176,901</span> (691.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m176,901\u001b[0m (691.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_emb_not_trainable.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "uL1x8Tiueb_l",
        "outputId": "bf7385f6-1261-4ad0-a247-5f9239e722bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)               │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m)               │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_trainable.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0_RrsTZsaKM"
      },
      "source": [
        "*   Comparar con los modelos anteriores en cuanto al desempeño, la cantidad de parámetros y el tiempo de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOsY688QqW8l"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dictionary[0] = \"<pad>\"\n",
        "\n",
        "\n",
        "exp.log_embedding(vectors= embedding_matrix , labels= dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLwg3ZMKC9sx"
      },
      "source": [
        "Respuesta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af_D4NTCsezj"
      },
      "source": [
        "*   Visualizar cómo es el espacio de *embedding*. ¿Qué diferencias puede observar con respecto al anterior?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX881l0X2PuU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tsZq6m2FMjv"
      },
      "source": [
        "Respuesta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnyfpJny_Eq-"
      },
      "source": [
        "# Parte 4: Mejoras en los Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsXZdnYo-sl"
      },
      "source": [
        "Se sugieren algunas líneas que podrían mejorar los desempeños obtenidos en las partes anteriores:\n",
        "\n",
        "* Ampliar el tamaño del vocabulario utilizado durante el entrenamiento, junto con aumentar la complejidad del modelo.\n",
        "* Implementar una función de preprocesamiento de texto que realice tareas como eliminar las etiquetas HTML, eliminar números y llevar a cabo otras acciones relevantes.\n",
        "* Considerar la utilización de n-gramas y/o modificar el tokenizador utilizado.\n",
        "* Evaluar el cambio de las neuronas recurrentes de GRU a LSTM.\n",
        "\n",
        "Piense por qué tiene sentido explorar estas estrategias y pruebe alguna que considere relevante o proponga alguna otra que le parezca relevante.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw28DePgC9sx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Eliminar HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Eliminar números\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Eliminar puntuación\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-fwZvyPiS6Y"
      },
      "outputs": [],
      "source": [
        "data_train_clean = [(clean_text(review.numpy().decode()), label.numpy()) for review, label in train_set.unbatch()]\n",
        "data_valid_clean = [(clean_text(review.numpy().decode()), label.numpy()) for review, label in valid_set.unbatch()]\n",
        "data_test_clean  = [(clean_text(review.numpy().decode()), label.numpy()) for review, label in test_set.unbatch()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onszm5bgjxm4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data_train_clean_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    [x for x, y in data_train_clean]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFkBtFk5jWSg"
      },
      "outputs": [],
      "source": [
        "vocab_size2 = 20000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size2)\n",
        "text_vec_layer.adapt(data_train_clean_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHhLgamAmLVR",
        "outputId": "11abb6c2-cfd3-4a75-ca9d-2e86a169c0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 19246 words (754 misses)\n"
          ]
        }
      ],
      "source": [
        "dictionary = text_vec_layer.get_vocabulary()\n",
        "word_index = dict(zip(dictionary, range(len(dictionary))))\n",
        "embedding_dim = embedding_matrix.shape[1]\n",
        "vocab_size = embedding_matrix.shape[0]\n",
        "\n",
        "\n",
        "num_tokens = len(dictionary) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embedding_layer_f = tf.keras.layers.Embedding(\n",
        "    input_dim=num_tokens,\n",
        "    output_dim=embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=True,\n",
        "    mask_zero=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN0v6ictnrd3",
        "outputId": "f470a553-26ed-4d09-d36a-ca67de2d27c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.8714 - loss: 0.2937 - val_accuracy: 0.8648 - val_loss: 0.3135\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.8818 - loss: 0.2782 - val_accuracy: 0.8700 - val_loss: 0.3050\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8868 - loss: 0.2665 - val_accuracy: 0.8700 - val_loss: 0.3079\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.8946 - loss: 0.2555 - val_accuracy: 0.8682 - val_loss: 0.3106\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 37ms/step - accuracy: 0.8918 - loss: 0.2531 - val_accuracy: 0.8814 - val_loss: 0.2876\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9010 - loss: 0.2364 - val_accuracy: 0.8628 - val_loss: 0.3141\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9025 - loss: 0.2292 - val_accuracy: 0.8764 - val_loss: 0.2953\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 37ms/step - accuracy: 0.9045 - loss: 0.2275 - val_accuracy: 0.8710 - val_loss: 0.3052\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 32ms/step - accuracy: 0.9102 - loss: 0.2162 - val_accuracy: 0.8774 - val_loss: 0.3038\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9156 - loss: 0.2055 - val_accuracy: 0.8766 - val_loss: 0.2977\n"
          ]
        }
      ],
      "source": [
        "embedding_layer_f.build((1,))\n",
        "embedding_layer_f.set_weights([embedding_matrix])\n",
        "\n",
        "\n",
        "model_trainable = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    embedding_layer_f,\n",
        "    tf.keras.layers.GRU(128, dropout = 0.4),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_trainable.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model_emb_not_trainable.fit(train_set, validation_data=valid_set, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yNw1VTGmvsLi",
        "outputId": "063068ac-2363-4c58-c5ef-e723c6b1f150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "model_trainable.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "texts_train, labels_train = zip(*data_train_clean)\n",
        "texts_valid, labels_valid = zip(*data_valid_clean)\n",
        "\n",
        "history = model_emb_not_trainable.fit(\n",
        "    x=list(texts_train),\n",
        "    y=np.array(labels_train),\n",
        "    validation_data=(list(texts_valid), np.array(labels_valid)),\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeAZ2nRRpP7W"
      },
      "outputs": [],
      "source": [
        "for text_tensor in data_train_clean_ds.take(5):\n",
        "    print(text_tensor.numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90O_LXGExzWp"
      },
      "source": [
        "# Parte 5: Desarrollo de una aplicación web (Opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAIXZ-_DNeN7"
      },
      "source": [
        "En esta parte veremos cómo generar una aplicación web sencilla que permita mostrar el funcionamiento de un modelo que hayamos entrenado. Para ello utilizaremos la biblioteca [Streamlit](https://streamlit.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz7kKyFaF6uQ"
      },
      "source": [
        "## Streamlit\n",
        "Streamlit es una biblioteca de código abierto escrita en Python que permite crear y compartir aplicaciones web que usan algoritmos de aprendizaje automático. En la [documentación](https://docs.streamlit.io/) encontrará información sobre cómo instalar y crear aplicaciones utilizando la biblioteca. El flujo de trabajo básico consta de los siguientes pasos:   \n",
        "\n",
        "1. Instalación   \n",
        "2. Desarrollo de la aplicación   \n",
        "3. Desplieque de la aplicación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_y-rIEzF6uQ"
      },
      "source": [
        "### Instalación\n",
        "En la mayoría de los casos, la biblioteca debería quedar instalada luego de crear un ambiente virtual (por ejemplo de conda) y hacer:   \n",
        "\n",
        "`pip install streamlit`  \n",
        "\n",
        "Puede verificar que la instalación sea correcta haciendo:    \n",
        "\n",
        "`streamlit hello`\n",
        "\n",
        "Puede ver los detalles de instalación en https://docs.streamlit.io/library/get-started/installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACy7SerfF6uR"
      },
      "source": [
        "### Desarrollo de la aplicación  \n",
        "\n",
        "Se sugiere desarrollar la aplicación partiendo de un ejemplo que clasifica críticas de cine utilizando un modelo entrenado con las técnicas vistas en el Taller 2. Dicho ejemplo puede verse en funcionamiento [acá](https://share.streamlit.io/taa-fing/taa-2022/main/apps/movie_review_app/movie_review_app.py). La siguiente celda descarga el código fuente y lo descomprime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s5kTwpCF6uR"
      },
      "outputs": [],
      "source": [
        "!wget iie.fing.edu.uy/~carbajal/movie_review/apps.zip\n",
        "!unzip apps.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDw4xQQ_F6uR"
      },
      "source": [
        "Copie el archivo *movie_review_app.py*, modifique el nombre, y realice las siguientes modificaciones (además de las de diseño que crea conveniente):   \n",
        "\n",
        "**Cambio de Modelo:**  Para hacer inferencia fuera de este *Notebook* será necesario contar con el modelo entrenado. El modelo se puede guardar con alguna de las técnicas vistas en el curso. Se sugiere utilizar el *Callback* **ModelCheckpoint**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYAdJltN6iWw"
      },
      "source": [
        "**Modificación del pipeline de inferencia:**  El objetivo es generar una función que a partir de una única reseña prediga si la reseña es *positiva*(1) o *negativa*(0). Para ello se brinda una función a completar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWkkHf_67KPS"
      },
      "outputs": [],
      "source": [
        "# Pipeline funtion to make inference\n",
        "def pipeline_inference (review, model):\n",
        "\n",
        "  '''\n",
        "  Función que prepara una review aislada y hace inferencia con el modelo.\n",
        "\n",
        "  Entradas:\n",
        "    review: String con la review a hacer inferencia\n",
        "    model: Modelo entrenado con el que se realiza inferencia\n",
        "\n",
        "  Salida:\n",
        "\n",
        "    pred: Predicción del modelo\n",
        "\n",
        "  '''\n",
        "\n",
        "  # ...\n",
        "\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_2uml4vSIvi"
      },
      "source": [
        "* Las siguientes celdas prueban la función. Verifique que funciona correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN9RStj0DQD_"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "checkpoint_filepath = '/content/...'\n",
        "\n",
        "model_loaded = keras.models.load_model(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLSyMO1BMuX"
      },
      "outputs": [],
      "source": [
        "review = 'This movie is really boring. I do not recommend it.'\n",
        "\n",
        "pred = pipeline_inference(review, model_loaded)\n",
        "\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmV5G1WRTDpi"
      },
      "outputs": [],
      "source": [
        "review = 'This movie is wonderful. I love it.'\n",
        "\n",
        "pred = pipeline_inference(review, model_loaded)\n",
        "\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1B8l8vF6uT"
      },
      "source": [
        "### Correr la aplicación localmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbsE4S8VF6uT"
      },
      "source": [
        "Una vez realizadas las modificaciones en el archivo principal, cree un directorio donde guardar los archivos de su aplicación. Guarde allí el modelo, el archivo tipo numpy con las palabras y su archivo principal (Ej. *movie_review_app.py*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xm1pGUmbV6_"
      },
      "source": [
        "* Una vez modificado el código, puede probarlo localmente. Para ello ejecute el siguiente comando, sustituyendo *movie_review_app.py* por el nombre de su archivo principal.\n",
        "\n",
        "\n",
        "`!streamlit run apps/movie_review_app/movie_review_app.py`    \n",
        "\n",
        "* Si en vez de localmente, está corriendo el notebook en Colab, ejecute:\n",
        "\n",
        "`!streamlit run apps/movie_review_app/movie_review_app.py & npx localtunnel --port 80`      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIsigqf7F6uU"
      },
      "source": [
        "### Despliegue de la aplicación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKXAqZA4uAYA"
      },
      "source": [
        "Una vez que la app fue desarrollada es posible compartirla para que otros puedan probarla. Para ello es necesario:  \n",
        "\n",
        "1. Contar con una cuenta de [Streamlit Cloud](https://docs.streamlit.io/streamlit-cloud/get-started#sign-up-for-streamlit-cloud) y un repositorio de GitHub donde almacenar el código.\n",
        "2.  Subir al repositorio  el código y los datos necesarios para correrlo.\n",
        "3. [Conectar la cuenta de Streamlit Cloud con la del repositorio](https://docs.streamlit.io/streamlit-cloud/get-started#connect-your-github-account).     \n",
        "4. [Publicar la app](https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN0y6d6rvO_u"
      },
      "source": [
        "*Comentario:* Puede que sea necesario agregar en el repositorio un archivo *requirements.txt* donde deba especificar las liberías utilizas en el archivo *main.py*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu0Q1R8L8sv5"
      },
      "source": [
        "# Parte 6: Uso de la liberería HuggingFace (Opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kmzCN_m_Fj-"
      },
      "source": [
        "Hugging Face es una organización de desarrolladores que utilizan una [plataforma](https://huggingface.co/) para compartir y desarrollar modelos de código abierto con arquitectura de Transformers, centrados principalmente en procesamiento de lenguaje natural, aunque también cuentan con modelos para procesamiento de imágenes y señales de audio.  \n",
        "\n",
        "Los modelos son desarrollados con técnicas del estado del arte, y entrenados con grandes volumnes de datos y capacidad de computo. Cualquier persona que quiera hacer uso de los modelos pre-entrenados puede hacerlo simplemente instalando la biblioteca de Transformers y los módulos necesarios para las tareas a implementar.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka4Wb85h_Yh7"
      },
      "source": [
        "## Uso de modelo preentrenado para hacer inferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzwVV4xzdUCP"
      },
      "source": [
        "Este código utiliza la biblioteca de HuggingFace \"transformers\" para importar y utilizar un modelo preentrenado de análisis de sentimientos. En este caso, se utiliza el modelo \"sentiment-analysis\", que se encarga de clasificar el sentimiento de un texto dado como positivo o negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTV-xjTz-Hp4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U transformers\n",
        "    %pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNHpyRbEdzBe"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"The movie was horrible.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "790-Oe-ud1kQ"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOanaaqueU_M"
      },
      "source": [
        "Utilizando este modelo realice inferencia sobre el conjunto de test, y evalue el desempeño obtenido. Compárelo con los obtenidos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKGWhmHYeOY5"
      },
      "outputs": [],
      "source": [
        "N_test = X_test.shape[0]\n",
        "\n",
        "pred_test = np.zeros(N_test)\n",
        "\n",
        "for i in range(N_test):\n",
        "  result = classifier(X_test[i][:1500]) # Corta la review a 1500 caracteres para adaptarse al modelo\n",
        "  if result[0]['label'] == 'NEGATIVE':\n",
        "    pred_test[i] = 1 - result[0]['score']\n",
        "  else:\n",
        "    pred_test[i] = result[0]['score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIOuSZhweSRI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test, pred_test > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N0puYDqewvQ"
      },
      "source": [
        "## Fine-tuning de un modelo ya pre-entrenado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgvNcM5c5OvE"
      },
      "source": [
        "Para ejecutar esta parte del notebook, deberá reiniciar el entorno de ejecución.\n",
        "\n",
        "La siguiente celda instala las bibliotecas necesarias para esta parte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-8opdWUlX6J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Instalamos la biblioteca con los modelos de arquitectura Transformer\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG38vGCE5RDr"
      },
      "source": [
        "El objetivo será realizar un ajuste de parámetros al modelo utilizando los datos del problema de interés, por lo tanto la siguiente celda levanta nuevamente los datos de la críticas de cine IMBd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qj_Q1RwtZdQ"
      },
      "outputs": [],
      "source": [
        "# Se levanta como pandas DataFrame (Se requiere tener el csv descargado de la página de Kaggle)\n",
        "data_file = 'IMDB Dataset.csv'\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "# Reduce el largo de las reviews a 1500 caracteres\n",
        "data['review'] = data['review'].str[:1500]\n",
        "\n",
        "# Convierte las etiquetas de sentimiento a 1 y 0\n",
        "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Print los datos actualizados\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ6YAawCuWEk"
      },
      "outputs": [],
      "source": [
        "data.iloc[0]['review']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ep8sEu5SGD"
      },
      "source": [
        "**Armado de Conjuntos** Se arman los datasets de Torch para realizar el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKy2juiLjJy4"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "# Convertir los pandas DataFrames a datasets\n",
        "train_data = datasets.Dataset.from_pandas(data[:20000]) # Se cortan la cantidad de reseñas para reducir el costo computacional de la ejecucción.\n",
        "val_data = datasets.Dataset.from_pandas(data[30000:35000])\n",
        "test_data = datasets.Dataset.from_pandas(data[35000-1:])\n",
        "\n",
        "# Renombrar las columnas para que coincidan con los nombres esperados por el modelo\n",
        "train_data = train_data.rename_column(\"sentiment\", \"label\")\n",
        "val_data = val_data.rename_column(\"sentiment\", \"label\")\n",
        "test_data = test_data.rename_column(\"sentiment\", \"label\")\n",
        "train_data = train_data.rename_column(\"review\", \"text\")\n",
        "val_data = val_data.rename_column(\"review\", \"text\")\n",
        "test_data = test_data.rename_column(\"review\", \"text\")\n",
        "\n",
        "# Crear un DatasetDict\n",
        "emotions = datasets.DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": val_data,\n",
        "    \"test\": test_data\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dKL_eCUiG6f"
      },
      "outputs": [],
      "source": [
        "train_ds = emotions[\"train\"]\n",
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFIaFB7sif7H"
      },
      "outputs": [],
      "source": [
        "train_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlDD4_jp5TnK"
      },
      "source": [
        "**Preparación de los datos** Los datos se pasan por el mismo tokenizer que se uso para el modelo de base que se utilizará. HuggingFace tiene un método \"AutoTokenizer\" para esto (simplemente determina cual es el tokenizer para ese modelo y lo aplica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOQqhbVZfWtY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt='distilbert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KylIPaQB5eGM"
      },
      "source": [
        "Finalmente implementamos una función para aplicar el tokenizer a todo el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHDvzHjnfnlQ"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "  return tokenizer(batch['text'], padding=True,truncation=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLuPJDbKfuCF"
      },
      "outputs": [],
      "source": [
        "emotions_encoded = emotions.map(tokenize,batched=True,batch_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKjAbs_5mVm"
      },
      "source": [
        " **Fine-tuning de un modelo**\n",
        "\n",
        " En primer lugar se levanta el modelo a ajustar y luego se procede a entrenarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VJz5QMlf0fX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_labels = 6\n",
        "\n",
        "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt,num_labels = num_labels).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Impwq8SGkmzX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  f1 = f1_score(labels,preds,average=\"weighted\")\n",
        "  acc = accuracy_score(labels,preds)\n",
        "  return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU5r9XPZkpBN"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "batch_size = 32\n",
        "logging_steps = len(emotions_encoded[\"train\"])\n",
        "model_name = f\"{model_ckpt}-finetuned-emotions\"\n",
        "training_args = TrainingArguments(output_dir = model_name,\n",
        "                                  num_train_epochs = 2,\n",
        "                                  learning_rate= 2e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  push_to_hub=False,\n",
        "                                  log_level=\"error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38qeDwU7kqzJ"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  compute_metrics = compute_metrics,\n",
        "                  train_dataset=emotions_encoded[\"train\"],\n",
        "                  eval_dataset = emotions_encoded[\"validation\"],\n",
        "                  tokenizer=tokenizer)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFoa3TEMlsQo"
      },
      "outputs": [],
      "source": [
        "preds_output = trainer.predict(emotions_encoded[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRC_QLvilug3"
      },
      "outputs": [],
      "source": [
        "preds_output.metrics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SUfgw7Siq3td",
        "rY6vz2Ekj8ig",
        "xVgxoLgl1-KA",
        "90O_LXGExzWp",
        "wu0Q1R8L8sv5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}