{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Aprendizaje Automático y Reconocimiento de Patrones\n",
    "***\n",
    "# Práctico 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Implementar desde cero una red neuronal de dos capas\n",
    "- Utilizar la implementación para:\n",
    "    - Clasificar datos sintéticos \n",
    "    - Clasificar entre imágenes de gatos y no gatos\n",
    "- Adquirir cierta intuición sobre las fronteras de decisión determinadas por una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de ejercicios\n",
    "\n",
    "[Ejercicio 1](#Ejercicio1): implementación de una red neuronal de dos capas       \n",
    "[Ejercicio 2](#Ejercicio2): jugando con tensorflow playground   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se importan las bibliotecas que se utilizarán\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fuaa_utils as fuaa\n",
    "\n",
    "# Ajustar parámetros de matplotlib\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Implementación de una red neuronal de dos capas\n",
    "<a id=\"Ejercicio1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notación\n",
    "\n",
    "- En general, la arquitectura de una red de $L$ capas de define mediante el vector $\\mathbf{d}=[d^{(0)},d^{(1)},\\ldots, d^{(L)}]$, siendo $d^{(l)}$ el número de nodos de la capa $l$.\n",
    "- En una red neuronal de dos capas, es decir con $L=2$, con $l=0$ se referirá a la entrada (no se la considera una capa) y con $l=2$ se referirá a la capa de salida.     \n",
    "- A la señal de entrada a la activación $j$ de la capa $l$ se le llamará $s_j^{(l)}$ y a la de salida $x_j^{(l)}$. Así, por ejemplo:\n",
    "    - La coordenada $j$ del vector de características $\\mathbf{x^{(0)}}$ es $x_j^{(0)}$     \n",
    "    - La coordenada $j$ del vector de salida $\\mathbf{x^{(L)}}$ es $x_j^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementarán algunos de los elementos constitutivos de una red neuronal que más se utilizan en la práctica.  Con ellos se construirán un par de redes neuronales de dos capas. En este ejercicio se limitará la profundidad de la red a 2 para evitar entrar en los detalles de implementación propios de una red muy general. No obstante, los bloques a implementar se diseñaron de forma que conceptualmente sean similares a los que se utilizarían en una red neuronal más profunda. A continuación se muestra un diagrama de bloques de la red neuronal que se implementará y se describen los distintos bloques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/diagrama_de_bloques.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Inicializar parámetros:** Inicializa los parámentros de la red. A los pesos de la capa $l$ de la red le llamaremos $W_l$, $b_l$ con $l=1,2$.\n",
    "- **Propagación hacia adelante:** La *propagación hacia adelante*  o *forward propagation* consiste en estimar la salida de la red a partir de la entrada. Cada nodo o capa de la red tiene un método *forward* asociado. En este ejercicio se implementarán los métodos forward asociados a los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activacion \n",
    "        - Afin --> Activacion\n",
    "- **Loss:** Calcula el valor de la función de costo a optimizar. Se implementarán dos funciones de costo:\n",
    "        - Entropía cruzada\n",
    "        - Error cuadrático medio\n",
    "- **Propagación hacia atrás:** Durante la *propagación hacia atrás* o *backpropagation* se calculan los gradientes necesarios para actualizar los parámetros de la red. Se implementarán métodos *backward* para los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activacion \n",
    "        - Afin --> Activacion\n",
    "- **Update:** Es el boque encargado de actualizar los parámetros. Para ello utiliza los gradientes calculados durante la *propagación hacia atrás* y un método de optimización. En este práctico se utilizará *descenso por gradiente* como método de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bloque de Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará el bloque de inicialización para el caso de una red neuronal de dos capas con la siguiente estructura:\n",
    "        \n",
    "        Afin --> Activacion 1 --> Afin --> Activacion 2\n",
    "\n",
    "**Ejercicio:** Completar la implementación de `inicializar_pesos()`. Los pesos $W_l$ serán inicializados en valores aleatorios pequeños. Los pesos correspondientes a términos de *bias* se inicializarán a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inicializar_pesos(d_0, d_1, d_2, tipo=\"aleatoria\", std_ruido=0.01, semilla=1):\n",
    "    \"\"\"\n",
    "    Entrada:\n",
    "        d_0: dimensión del vector de características\n",
    "        d_1: número de nodos de la capa oculta\n",
    "        d_2: número de nodos de la capa de salida\n",
    "        tipo: string que indica el tipo de inicialización a utilizar en los pesos\n",
    "              'aleatoria': inicializa los pesos a valores aleatorios con distribución\n",
    "                           gaussiana. Los términos de bias se inicializan a cero.\n",
    "        std_ruido: desviación estándar del ruido gaussiano\n",
    "        semilla: semilla a utilizar para generar los valores aleatorios\n",
    "\n",
    "    Salida:\n",
    "        diccionario con los siguientes pares clave-valor:\n",
    "            W1: matriz de pesos de la capa 1 de tamaño (d_0, d_1)\n",
    "            b1: vector de bias de la capa 1 de tamaño (d_1)\n",
    "            W2: matriz de pesos de capa 2 de tamaño (d_1, d_2)\n",
    "            b2: vector de bias de la capa 2 de tamaño (d_2)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(semilla)\n",
    "\n",
    "    if tipo == \"aleatoria\":\n",
    "\n",
    "        #####################################################\n",
    "        ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "        \n",
    "        W1 = np.random.normal(0,std_ruido,(d_0,d_1))\n",
    "        W2 = np.random.normal(0,std_ruido,(d_1,d_2)) \n",
    "        b1 = np.zeros(d_1)\n",
    "        b2 = np.zeros(d_2)\n",
    "\n",
    "        #####################################################\n",
    "        ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "    # Se genera el diccionario con los valores inicializados\n",
    "    parametros = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de W1 validadas.\u001b[0m\n",
      " \u001b[32mDimensiones de b1 validadas.\u001b[0m\n",
      " \u001b[32mDimensiones de W2 validadas.\u001b[0m\n",
      " \u001b[32mDimensiones de b2 validadas.\u001b[0m\n",
      " \u001b[32mCálculo de W1 validado.\u001b[0m\n",
      " \u001b[32mCálculo de b1 validado.\u001b[0m\n",
      " \u001b[32mCálculo de W2 validado.\u001b[0m\n",
      " \u001b[32mCálculo de b2 validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar inicializar_pesos()\n",
    "fuaa.validar_resultado(\"inicializar_pesos\", funcion=inicializar_pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación:** *La inicialización de pesos en este caso fue aleatoria. Con redes de pocas capas esta inicialización suele producir buenos resultados. Sin embargo, a medida que aumenta la profundidad de la red la correcta inicialización de los parámetros adquiere una relevancia mayor. Quien quiera profundizar en la importancia de la inicialización en el caso de redes profundas puede consultar:* \n",
    "\n",
    "* [Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of\n",
    "    training deep feedforward neural networks.” International Conference on Artificial Intelligence and Statistics. 2010.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "* [He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level\n",
    "    performance on imagenet classification.” arXiv preprint arXiv:1502.01852 (2015).](https://arxiv.org/pdf/1502.01852v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bloques Forward\n",
    "\n",
    "En esta sección se implementarán los métodos *forward* de los componentes de la red neuronal de dos capas que vamos a utilizar. Observar que la capa oculta tiene un número $d^{(1)}$ de nodos mientras que la de salida tiene 1 un solo nodo. Además observar que en cada nodo se realiza una *transformación afin* de los datos de entrada y luego se pasa el resultado por una *función de activación*.\n",
    "\n",
    "<img src=\"img/red_dos_capas.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular, se implementarán los métodos *forward* de los siguientes bloques: \n",
    "\n",
    "- Bloque `Afín`\n",
    "- Bloque `Activación` donde la activación puede ser ReLU, Sigmoide o TangenteH.\n",
    "- Bloque `Afín -> Activación`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `Afin` forward \n",
    "\n",
    "La señal de entrada a la activación de la capa $\\textit{l}$ puede escribirse como:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{s}^{(l)}$ y $\\mathbf{b}^{(l)}$ son vectores de tamaño $d^{(l)}$, $\\mathbf{x}^{(l-1)}$  es un vector de tamaño $d^{(l-1)}$ y $W^{(l)}$ es una matriz de tamaño $d^{(l-1)} \\times d^{(l)}$.\n",
    "\n",
    "**Observación:** *Esta forma de escribir $\\mathbf{s}^{(l)}$ es levemente distinta a la expresión del libro, en la que todos los parámetros de la capa se agrupan en una matriz $W^{(l)}$ de tamaño $d^{(l-1)+1} \\times d^{(l)}$. En esta expresión se desacoplan los parámetros de bias y se indican mediante el vector $\\mathbf{b}^{(l)}$. Una ventaja práctica que tiene desacoplar los parámetros de bias es que $\\mathbf{x}^{(l)}=\\theta \\left(\\mathbf{s}^{(l)}\\right) $ en vez de la expresión en coordenadas homogéneas $\\mathbf{x}^{(l)}= \\left[ 1 , \\left( \\theta \\left(\\mathbf{s}^{(l)}\\right) \\right)^T \\right]^T $ que utiliza el libro del curso.*\n",
    "\n",
    "La ecuación (1) es válida cuando la entrada a la capa es un único vector $\\mathbf{x}^{(l-1)}$. En la práctica es más habitual procesar un $\\textit{batch}$ de vectores de entrada a la vez, por lo tanto es deseable contar con una expresión que genere la salida para todos los vectores de entrada a la vez. Al evitar la utilización de un bloque $\\textit{for}$ que itere por cada una de las muestras del $\\textit{batch}$ se mejora la eficiencia de la implementación.   \n",
    "\n",
    "\n",
    "La versión de la ecuación (1) que actúa sobre un conjunto de muestras a la vez es la siguiente:\n",
    "\n",
    "$$\n",
    "S^{(l)} = X^{(l-1)}W^{(l)} + \\mathbf{b}^{(l)} \\tag{2}\n",
    "$$\n",
    "\n",
    "donde $X^{[0]} = X$, siendo $X$ una matriz que contiene *un vector de características en cada fila*.\n",
    "\n",
    "**Ejercicio**: Implementar el método `afin_forward()` utilizando la versión eficiente dada por la ecuación (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def afin_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante en una capa afin.\n",
    "\n",
    "    Entrada:\n",
    "        X: matriz de tamaño (N, dim_capa_anterior) que en cada fila contiene\n",
    "           un vector de activaciones de la capa anterior (o datos de\n",
    "           entrada)\n",
    "        W: matriz de pesos de tamaño (dim_capa_anterior, dim_capa_actual)\n",
    "        b: vector de bias de tamaño (dim_capa_actual,)\n",
    "\n",
    "    Salida:\n",
    "        S: matriz de tamaño (N, dim_capa_actual) que contiene los scores o\n",
    "           señal de entrada a la activación\n",
    "        cache: (X, W, b) tupla que contiene X, W y b. Son almacenados para\n",
    "               calcular el paso backward eficientemente\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    S = np.dot(X,W) + b\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    cache = (X, W, b)\n",
    "\n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensión de la salida validada.\u001b[0m\n",
      " \u001b[32mResultado validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar afin_forward (solo la salida)\n",
    "fuaa.validar_resultado(\"afin_forward\", funcion=afin_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se implementarán tres de las funciones de activación más utilizadas, con $S$ dado por la ecuación (2):\n",
    "\n",
    "- **Sigmoide**: $\\sigma(S) = \\frac{1}{ 1 + e^{-S}}$. Esta función devuelve, además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiliza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = sigmoid(S)\n",
    "```\n",
    "\n",
    "- **Tangente Hiperbólica**: $\\tanh(S) = \\frac{e^S-e^{-S}}{e^S+e^{-S}}$. Esta función devuelve, además del resultado de *np.tanh(S)*, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).       \n",
    "\n",
    "``` python\n",
    "X, cache = tanh(S)\n",
    "```\n",
    "\n",
    "\n",
    "- **Rectified Linear Unit**:  $ReLU(S) = \\max(0, S)$.  Esta función devuelve además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = relu(S)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoide(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación.\n",
    "           Las dimensiones de entrada no están definidas.\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de sigmoid(S)\n",
    "        cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    cache = S\n",
    "\n",
    "    X = 1 / (1+ np.exp(-S))\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de la salida validadas.\u001b[0m\n",
      " \u001b[32mSalida \"cache\" validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar sigmoide.\n",
    "fuaa.validar_resultado(\"sigmoide\", funcion=sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tanh(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación tangente hiperbólica\n",
    "\n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación.\n",
    "           Las dimensiones de entrada no están definidas.\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de tanh(S)\n",
    "        cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    cache = S\n",
    "\n",
    "    X = (np.exp(S)-np.exp(-S))/(np.exp(S)+np.exp(-S))\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de la salida validadas.\u001b[0m\n",
      " \u001b[32mSalida \"cache\" validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar la tangente hiperbólica.\n",
    "fuaa.validar_resultado(\"tanh\", funcion=tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación relu\n",
    "\n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación.\n",
    "           Las dimensiones de entrada no están definidas.\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de relu(S)\n",
    "        cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    cache = S\n",
    "\n",
    "    X= np.maximum(np.zeros_like(S),S)\n",
    "  \n",
    "       \n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de la salida validadas.\u001b[0m\n",
      " \u001b[32mSalida \"cache\" validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar la función Rectified Linear Unit.\n",
    "fuaa.validar_resultado(\"relu\", funcion=relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Aplicación conjunta de capa afín y activación: `Afin --> Activacion` forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se implementan redes profundas es conveniente agrupar varias funciones en una sola operación. Para ejemplificar, en este caso crearemos una capa que aplique la transformación afín y una de las activaciones disponibles. \n",
    "\n",
    "**Ejercicio**: Implementar la propagación hacia adelante de una capa `Afin --> Activacion`. El método `afin_activacion_forward()` implementa la operación:\n",
    "\n",
    "$$\n",
    "X^{(l)} = \\theta(S^{(l)}) = \\theta(X^{(l-1)}W^{(l)} +b^{(l)})\n",
    "$$\n",
    "\n",
    "donde la activación $\\theta(\\cdot)$ será alguna de las implementadas. Se deberá hacer uso de `afin_forward()` y de la función de activación pasada como argumento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def afin_activacion_forward(X_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante para una capa Afin-->Activación.\n",
    "\n",
    "    Entrada:\n",
    "        X_prev: arreglo de tamaño (N, dim_capa_anterior) que contiene la\n",
    "                activación de la capa anterior (o datos de entrada):\n",
    "        W: matriz de pesos de tamaño (dim_capa_anterior, dim_capa_actual)\n",
    "        b: vector de bias de tamaño (dim_capa_actual)\n",
    "        activacion: la activacion a utilizar en esta capa se indica con uno de los\n",
    "                    siguientes strings: 'sigmoide', 'tanh' o 'relu'\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo de tamaño (N, dim_capa_actual) que contiene la salida\n",
    "           de la función de activación\n",
    "        cache: tupla que contiene \"cache_afin\" y \"cache_activacion\".\n",
    "               Se almacenan para calcular la propagación hacia atrás eficientemente\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    S, cache_afin = afin_forward(X_prev,W,b)\n",
    "\n",
    "    if activacion == 'sigmoide':\n",
    "        X, cache_activacion = sigmoide(S)\n",
    "    elif activacion == 'tanh':\n",
    "        X, cache_activacion = tanh(S)\n",
    "    else:\n",
    "        X, cache_activacion = relu(S)\n",
    "        \n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    cache = (cache_afin, cache_activacion)\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de la salida sigmoide validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida sigmoide validado.\u001b[0m\n",
      " \u001b[32mDimensiones de la salida tanh validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida tanh validado.\u001b[0m\n",
      " \u001b[32mDimensiones de la salida relu validada.\u001b[0m\n",
      " \u001b[32mCálculo de la salida relu validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar la capa Afin->Activación\n",
    "fuaa.validar_resultado(\"afin_activacion_forward\", funcion=afin_activacion_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de costo\n",
    "\n",
    "En esta sección se implementarán dos de las funciones de costo más utilizadas.  \n",
    "\n",
    "- **Entropía cruzada:** Es la función de costo más utilizada en problemas de clasificación binaria. Se recuerda que la misma se define mediante la fórmula:\n",
    "$$\n",
    "H(\\mathbf{\\mathbf{x}^{(L)}}, \\mathbf{y})= -\\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( y_n\\log x^{(L)}_n + (1-y_n)\\log\\left(1- x^{(L)}_n\\right) \\right) \\tag{3}\n",
    "$$\n",
    "\n",
    "- **Error cuadrático medio:** Es la función de costo más utilizada en problemas de regresión. Se recuerda que la misma se define mediante la fórmula:\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf{x}^{(L)}, \\mathbf{y})= \\frac{1}{2N} \\sum\\limits_{n = 1}^{N} \\left(y_n - x^{(L)}_n \\right)^2 \\tag{4}\n",
    "$$\n",
    "\n",
    "**Ejercicio**: Implementar los método `mse()` y `entropia_cruzada()`. Observar que en ambos casos la función deberá devolver, además del costo, el gradiente del costo respecto al vector $\\mathbf{x}^{(L)}$ (salida de la red y entrada del bloque *Loss*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse(xL, y):\n",
    "    \"\"\"\n",
    "    Implementa el error cuadratico medio como función de costo de una red neuronal con\n",
    "    una sola capa de salida.\n",
    "\n",
    "    Entrada:\n",
    "        xL: vector de dimensión (N,1) que contiene las salidas generadas por la red\n",
    "            neuronal para N muestras.\n",
    "        y:  vector de dimensión (N,1) que contiene las salidas esperadas.\n",
    "\n",
    "    Salida:\n",
    "        costo: escalar con el costo calculado\n",
    "        dxL: gradiente del costo respecto a xL, tiene las mismas dimensiones que xL\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    costo = np.mean((y-xL)**2)/2\n",
    "\n",
    "    dxL = - ((y-xL))/N\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    # Para asegurarnos que la salida sea un escalar (Ej: transforma [[12]] en 12).\n",
    "    costo = np.squeeze(costo)\n",
    "\n",
    "    return costo, dxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de costo validadas.\u001b[0m\n",
      " \u001b[32mDimensiones de dxL validadas.\u001b[0m\n",
      " \u001b[32mCálculo del costo validado.\u001b[0m\n",
      " \u001b[32mCálculo del gradiente validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar error cuadratico medio como función de costo de una red neuronal.\n",
    "fuaa.validar_resultado(\"mse\", funcion=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropia_cruzada(xL, y):\n",
    "    \"\"\"\n",
    "    Implementa la entropía cruzada\n",
    "\n",
    "    Entrada:\n",
    "        xL: vector de dimensión (N,1) que contiene las \"probabilidades\"\n",
    "            de pertenecer a la clase positiva estimadas por el modelo\n",
    "        y: vector de etiquetas de dimesión (N,1) (con unos para la clase\n",
    "           positiva y 0 para la negativa)\n",
    "\n",
    "    Salida:\n",
    "        costo: escalar con el costo calculado\n",
    "        dxL: gradiente del costo respecto a xL, tiene las mismas dimensiones que xL\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    costo = - np.mean(y*np.log(xL) + (1-y)*np.log(1-xL))\n",
    "\n",
    "    dxL = -( y/xL - ((1-y)/(1-xL)) )/N\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    # Para asegurarnos que la salida sea un escalar (Ej: transforma [[12]] en 12).\n",
    "    costo = np.squeeze(costo)\n",
    "    return costo, dxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones de costo validadas.\u001b[0m\n",
      " \u001b[32mDimensiones de dxL validadas.\u001b[0m\n",
      " \u001b[32mCálculo del costo validado.\u001b[0m\n",
      " \u001b[32mCálculo del gradiente validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar la entropia_cruzada.\n",
    "fuaa.validar_resultado(\"entropia_cruzada\", funcion=entropia_cruzada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Propagación hacia atrás\n",
    "\n",
    "Al igual que como se hizo con la propagación hacia adelante, se implementarán funciones de ayuda para realizar la propagación hacia atrás. Se recuerda que la propagación hacia atrás se utiliza para calcular el gradiente de la función de costo respecto a los parámetros de la red. \n",
    "\n",
    "<img src=\"img/diagrama_backpropagation.png\" style=\"width:8=1012px;height:223px;\">\n",
    "<caption><center>Propagación hacia adelante y atrás en una red de dos capas con arquitectura: <code>Afin --> Activacion 1 --> Afin --> Activacion 2</code><br></center></caption>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se implementará la versión *backward* de cada una de las funciones *forward* implementadas anteriormente. Es decir, se implementarán las siguientes funciones de propagación hacia atrás:\n",
    "- `afin_backward()`\n",
    "- `ACTIVACION_backward()` donde `ACTIVACION` puede ser *ReLU*, *sigmoide* o *tanh*\n",
    "- `afin_activacion_backward()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. `Afin` backward\n",
    "\n",
    "Durante la propagación hacia adelante en la capa $l$ (sin considerar la activación) se calcula para una muestra: \n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "Si se llama $e_n$ al costo debido a la muesta $n$ y se asume conocido el *vector de sensibilidad* $\\delta^{(l)}=\\frac{\\partial e_n}{\\partial \\mathbf{s}^{(l)}}$, en el teórico del curso se vio que \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{W^{(l)}}}=\\mathbf{x}^{(l-1)} \\left( \\delta^{(l)} \\right)^T\n",
    "$$\n",
    "\n",
    "Análogamente a como se hizo en el caso de la propagación hacia adelante, si se considera la contribución al error de un conjunto de muestras a la vez la ecuación se puede escribir en forma vectorizada como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W^{(l)}}}= dW^{(l)} = \\left( X^{(l-1)}\\right)^ T dS^{(l)}   \\tag{5}\n",
    "$$\n",
    "\n",
    "donde $dS^{(l)}$ es una matríz de tamaño $N\\times d^{(l)}$ que en cada fila contiene el vector de sensibilidad $\\delta^{(l)}_n$ correspondiente a una de las muestras.\n",
    "\n",
    "Las derivadas respecto al vector de bias $\\mathbf{b}^{(l)}$ se calculan de forma similar (puede pensarse como un caso particular en que $X^{(l-1)}$ es un vector columna de unos) por lo que\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}= d\\mathbf{b}^{(l)} =\\mathbb{1} ^ T dS^{(l)}  \\tag{6}\n",
    "$$\n",
    "\n",
    "Finalmente se calcula la influencia de cada una de las características en el error. Considerando primero el caso de una muestra, se tiene que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{\\mathbf{x}^{(l-1)}}} = W^{(l)} \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "que en forma vectorizada puede escribirse como:\n",
    "\n",
    "$$ \n",
    " \\frac{\\partial E }{\\partial X^{(l-1)}} = dX^{(l-1)} = dS^{(l)} \\left( W^{(l) }\\right)^T \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Ejercicio**: Utilizando las ecuaciones (5),(6) y (7) implementar el método `afin_backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def afin_backward(dS, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una capa l (sin considerar la activación).\n",
    "\n",
    "    Entrada:\n",
    "        dS: Gradiente de la función de costo con respecto a la salida de la capa actual\n",
    "            (sin considerar la activación).\n",
    "        cache: tupla de valores (X_prev, W, b) calculados durante la propagación hacia\n",
    "               adelante de la capa actual.\n",
    "\n",
    "    Salida:\n",
    "        dX_prev: Gradiente de la función de costo con respecto a la activación de la capa\n",
    "                 anterior (l-1), tiene el mismo tamaño que X_prev.\n",
    "        dW: Gradiente de la función de costo con respecto a W (de la capa actual l),\n",
    "            tiene el mismo tamaño que W.\n",
    "        db: Gradiente de la función de costo con respecto a b (de la capa actual l),\n",
    "            tiene el mismo tamaño que b.\n",
    "    \"\"\"\n",
    "    X_prev, W, b = cache\n",
    "    N = X_prev.shape[0]\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    dW = X_prev.T @ dS\n",
    "\n",
    "    db = np.ones(N) @ dS\n",
    "\n",
    "    dX_prev = dS @ W.T\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensión del gradiente respecto a la activación de la capa anterior (dE/dX_prev) validado.\u001b[0m\n",
      " \u001b[32mDimensión del gradiente respecto a W de la capa actual(dE/dW) validado.\u001b[0m\n",
      " \u001b[32mDimensión del gradiente respecto a \"b\" de la capa actual (dE/db) validado.\u001b[0m\n",
      " \u001b[32mGradiente respecto a la activación de la capa anterior (dE/dX_prev) validado.\u001b[0m\n",
      " \u001b[32mGradiente respecto a W de la capa actual (dE/dW) validado.\u001b[0m\n",
      " \u001b[32mGradiente respecto a \"b\" de la capa actual (dE/db) validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar afin_backward().\n",
    "fuaa.validar_resultado(\"afin_backward\", funcion=afin_backward, f_forward=afin_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Activación backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si  $\\theta(\\cdot)$ es la función de activación, entonces tanto\n",
    "`sigmoide_backward()` como `tanh_backward()` y `relu_backward()` calculan \n",
    "\n",
    "$$\n",
    "dS^{(l)} = dX^{(l)} \\theta'(S^{(l)})   \\tag{8}\n",
    "$$\n",
    "\n",
    "donde $\\theta'(\\cdot)$ debe ser calculado para cada caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar los métodos *backward* cada una de las funciones de activación implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tanh_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación tanh().\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa tanh(),\n",
    "            el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Salida:\n",
    "        dS: gradiente del costo respecto a S.\n",
    "    \"\"\"\n",
    "\n",
    "    S = cache\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "    X , _ = tanh(S)\n",
    "    deri = 1 - X**2\n",
    "    dS = dX * deri\n",
    "\n",
    "    \n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones del gradiente respecto a S validadas.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar tanh_backward\n",
    "fuaa.validar_resultado(\"activacion_backward\", f_backward=tanh_backward, f_forward=tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoide_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa sigmoide(),\n",
    "            el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Salida:\n",
    "        dS: gradiente del costo respecto a S\n",
    "    \"\"\"\n",
    "\n",
    "    S = cache\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    deri = np.exp(S)/(np.exp(S)+1)**2\n",
    "    dS = dX * deri\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      " \u001b[32mDimensiones del gradiente respecto a S validadas.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar sigmoide_backward()\n",
    "fuaa.validar_resultado(\n",
    "    \"activacion_backward\", f_backward=sigmoide_backward, f_forward=sigmoide\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación ReLu.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "            el tamaño del arreglo no está definido.\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante.\n",
    "\n",
    "    Salida:\n",
    "        dS: gradiente del costo respecto a S.\n",
    "    \"\"\"\n",
    "\n",
    "    S = cache\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "    '''\n",
    "    dim = S.shape\n",
    "   \n",
    "    X = np.ones(dim)\n",
    "    \n",
    "    for i in range(dim[0]):\n",
    "        for j in range(dim[1]):\n",
    "            if S[i,j] <= 0:\n",
    "                X[i,j] = 0\n",
    "              \n",
    "    dS = dX*X\n",
    "    print(dX)\n",
    "    print(dS)\n",
    "    '''  \n",
    "    dS = dX.copy()\n",
    "    dS[S <= 0] = 0 \n",
    "    print(dS)\n",
    "    \n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      "[[-0.22557183 -1.1928829   0.          0.          0.          0.\n",
      "   0.         -0.35229594  0.          0.        ]\n",
      " [ 0.         -0.84338097  0.          0.85434757 -0.90377338 -1.0525584\n",
      "   0.          0.18083726 -0.4125417   1.22913948]\n",
      " [-0.97791748 -0.63978524 -0.00880963  0.36213294  0.35148162  0.\n",
      "  -0.84272962  0.         -2.39079478  0.88256212]\n",
      " [-1.12082008  0.12416778  0.         -1.62701704  0.          2.00862337\n",
      "   0.         -0.86943856  0.          0.        ]\n",
      " [ 0.          0.          0.         -0.67498124  0.         -0.11970425\n",
      "   0.59700586 -2.0399063   0.         -1.61700346]\n",
      " [ 0.          0.          1.97009461 -0.81967191  0.42367521 -1.83570698\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          2.13935149 -0.77429891  0.          0.          0.\n",
      "   0.          0.          0.59190367 -0.56016357]\n",
      " [ 0.          0.          0.          0.          0.37364743 -0.26051388\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.         -1.74506882 -0.2476718   0.\n",
      "   1.09734348  0.          0.         -0.32986357]\n",
      " [ 0.         -0.00396778  0.          0.          0.          0.\n",
      "  -0.9308637   0.          0.         -1.1058167 ]]\n",
      " \u001b[32mDimensiones del gradiente respecto a S validadas.\u001b[0m\n",
      " \u001b[32mCálculo de la salida validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar relu_backward()\n",
    "fuaa.validar_resultado(\"activacion_backward\", f_backward=relu_backward, f_forward=relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. `Afin --> Activacion` backward\n",
    "\n",
    "A continuación se implementará la función que realiza la propagación hacia atrás del la capa *Afin-->Activacion*. \n",
    "\n",
    "**Ejercicio**: Implementar la función `afin_activacion_backward()`. Para ello utilizar las funciones implementadas anteriormente: `afin_backward()` y la `ACTIVACION_backward()` que corresponda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def afin_activacion_backward(dX, cache, activacion):\n",
    "    \"\"\"\n",
    "    Implementar la propagación hacia atrás para la capa Afin --> Activacion.\n",
    "\n",
    "    Entradas:\n",
    "        dX: gradiente del costo respecto a la salida de la capa actual.\n",
    "        cache: tupla con los valores(cache_afin, cache_activacion).\n",
    "        activacion: la activación a utilizar en esta capa, puede ser 'sigmoide', 'tanh'\n",
    "                    o 'relu'.\n",
    "\n",
    "    Salidas:\n",
    "        dX_prev: gradiente del costo con respecto a la activación de la capa anterior(l-1),\n",
    "                 tiene las mismas dimensiones que X_prev.\n",
    "        dW: gradiente del costo con respecto a W (de la capa actual l), tiene las mismas\n",
    "            dimensiones que W.\n",
    "        db: gradiente del costo con respecto a b (de la capa actual l), tiene las mismas\n",
    "            dimensiones que b.\n",
    "    \"\"\"\n",
    "\n",
    "    cache_afin, cache_activacion = cache\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "    \n",
    "    if activacion == 'sigmoide':\n",
    "        dS = sigmoide_backward(dX,cache_activacion)\n",
    "    elif activacion == 'tanh':\n",
    "        dS = tanh_backward(dX,cache_activacion)\n",
    "    else:\n",
    "        dS = relu_backward(dX,cache_activacion)\n",
    "    \n",
    "    dX_prev, dW, db = afin_backward(dS,cache_afin) \n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|\u001b[34m FuAA: validar resultado                                                 \u001b[0m|\n",
      "+-------------------------------------------------------------------------+\n",
      "[[ 0.          0.          0.          0.          0.98687019  0.\n",
      "   0.119619    2.13935149  0.         -0.42011785]\n",
      " [ 0.         -1.0141586  -2.30296116  0.          0.          0.\n",
      "   0.          0.         -0.93426589  0.        ]]\n",
      " \u001b[32mCálculo del grandiente respecto a dX (relu) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a dW (relu) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a db (relu) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a dX (sigmoide) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a dW (sigmoide) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a db (sigmoide) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a dX (tanh) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a dW (tanh) validado.\u001b[0m\n",
      " \u001b[32mCálculo del grandiente respecto a db (tanh) validado.\u001b[0m\n",
      "+-------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Validar afin_activacion_backward()\n",
    "fuaa.validar_resultado(\n",
    "    \"afin_activacion_backward\",\n",
    "    f_backward=afin_activacion_backward,\n",
    "    f_forward=afin_activacion_forward,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Actualización de los parámetros\n",
    "\n",
    "En esta sección se actualizarán los parámetros del modelo mediante el método de *descenso por gradiente*:\n",
    "\n",
    "$$ W^{(l)} = W^{(l)} -\\eta \\text{ } dW^{(l)} \\tag{9}$$\n",
    "$$ \\mathbf{b}^{(l)} = \\mathbf{b}^{(l)} -\\eta \\text{ } \\mathbf{db}^{(l)} \\tag{10}$$\n",
    "\n",
    "donde $\\eta$ es el *learning rate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar `actualizar_parametros()` para actualizar los parámetros usando *descenso por gradiente*. Luego de actualizar los parámetros, almacenarlos en el diccionario de parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def actualizar_parametros(parametros, gradientes, learning_rate):\n",
    "    \"\"\"\n",
    "    Se actualizan los parámetros utilizando descenso por gradiente. Si bien en\n",
    "    este notebook se trabaja con una red de dos capas, el método se implementa\n",
    "    en forma genérica para mostrar como se haría en el caso más general.\n",
    "\n",
    "    Entrada:\n",
    "        parametros: diccionario de python que contiene los parámetros\n",
    "                        parametros[\"W\" + str(l)] = ...\n",
    "                        parametros[\"b\" + str(l)] = ...\n",
    "        gradientes: diccionario de python que contiene los gradientes (las\n",
    "                    salidas de los métodos backward)\n",
    "                        gradientes[\"W\" + str(l)] = ...\n",
    "                        gradientes[\"b\" + str(l)] = ...\n",
    "\n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros actualizados\n",
    "                    parametros[\"W\" + str(l)] = ...\n",
    "                    parametros[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parametros) // 2  # número de capas en la red neuronal\n",
    "\n",
    "    # Se actualiza cada uno de los parámetros. En el caso de una red profunda de\n",
    "    # L capas se hace con un loop que va recorriendo cada parámetro.\n",
    "    for l in range(1, L + 1):\n",
    "\n",
    "        #####################################################\n",
    "        ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "        # parametros\n",
    "        \n",
    "        #####################################################\n",
    "        ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clasificación utilizando datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan datos sintéticos con forma de flor pertenecientes a dos clases: $cero$ y $uno$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, Y = fuaa.generar_flor()\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se muestran los datos\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y.flatten(), s=40, cmap=plt.cm.Spectral)\n",
    "plt.tight_layout()\n",
    "plt.ylim([X[:,1].min() - 1, X[:, 1].max() + 1])\n",
    "plt.xlim([X[:,0].min() - 1, X[:, 0].max() + 1])\n",
    "plt.axis(\"square\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Red para clasificar los datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificar los datos sintéticos se utilizará una red de dos capas con la siguiente arquitectura:   \n",
    "\n",
    "    Afin --> Tanh --> Afin --> Sigmoide\n",
    "\n",
    "**Ejercicio:** Completar la implementación de los métodos `red_dos_capas_datos_sinteticos()` utilizando los métodos *forward* y *backward* adecuados para dicha arquitectura. Como función de costo se utilizará la *entropía cruzada*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def red_dos_capas_datos_sinteticos(\n",
    "    X, Y, dims_capas, num_iter=1000, learning_rate=1, mostrar_costo=False, semilla=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de dos capas: Afin->Tanh->Afin->Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        X: datos de entrada, de tamaño (N, d_0).\n",
    "        Y: etiquetas (1 para la clase positiva y 0 para la negativa), de tamaño (N,1).\n",
    "        dims_capas: dimensiones de las capas(d_0, d_1, d_2).\n",
    "        num_iter: número de iteraciones del loop de optimización.\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso\n",
    "                       por gradiente.\n",
    "        mostrar_costo: Si vale True, se muestra el costo cada 100 iteraciones.\n",
    "        semilla: semilla utilizada para la generación de números aleatorios.\n",
    "    Salida:\n",
    "        parametros: un diccionario de python que contiene W1, W2, b1, and b2.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(semilla)\n",
    "\n",
    "    # Se inicializa el diccionario que almacena los gradientes\n",
    "    gradientes = {}\n",
    "\n",
    "    # Lista que almacena el costo\n",
    "    costos = []\n",
    "\n",
    "    # Número de muestras\n",
    "    N = X.shape[0]\n",
    "\n",
    "    d_0, d_1, d_2 = dims_capas\n",
    "\n",
    "    # Se inicializan los parámetros del diccionario invocando a una de las funciones\n",
    "    # previamente implementadas.\n",
    "    parametros = inicializar_pesos(d_0, d_1, d_2, semilla=semilla)\n",
    "\n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "\n",
    "    # Loop (descenso por gradiente)\n",
    "    for i in range(0, num_iter):\n",
    "\n",
    "        #####################################################\n",
    "        ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "        # Propagación hacia adelante: Afin --> Tanh --> Afin --> Sigmoide.\n",
    "        # Entradas: \"X, W1, b1\". Salidas: \"X1, cache1, X2, cache2\".\n",
    "        # X1, cache1 =\n",
    "        # X2, cache2 =\n",
    "\n",
    "        # Se calcula el costo y se inicia la propagación hacia atrás\n",
    "        # costo, dX2 =\n",
    "\n",
    "        # Propagación hacia atrás.\n",
    "        # Entradas: \"dX2, cache2, cache1\". Salidas: \"dX1, dW2, db2, dW1, db1, dX0 (no utilizado)\".\n",
    "        # dX1, dW2, db2 =\n",
    "        # dX0, dW1, db1 =\n",
    "\n",
    "        # Se almacenan los gradientes recientemente calculados en el diccionario\n",
    "        # gradientes[\"W1\"] =\n",
    "        # gradientes[\"b1\"] =\n",
    "        # gradientes[\"W2\"] =\n",
    "        # gradientes[\"b2\"] =\n",
    "\n",
    "        # Se actualizan los parámetros\n",
    "        # parametros =\n",
    "\n",
    "        # Se obtienen los nuevos W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "        # W1 =\n",
    "        # b1 =\n",
    "        # W2 =\n",
    "        # b2 =\n",
    "\n",
    "        #####################################################\n",
    "        ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "        # Se guarda la evolución del costo cada Ni iteraciones y se muestra cada 10 x Ni.\n",
    "        Ni = 100\n",
    "        if mostrar_costo:\n",
    "            if i == 0:\n",
    "                print(\"+--------+---------+\")\n",
    "                print(\"|  iter  |  costo  |\")\n",
    "                print(\"+--------+---------+\")\n",
    "            if i % Ni == 0:\n",
    "                costos.append(costo)\n",
    "            if i % (10 * Ni) == 0:\n",
    "                print(\"| %6d | %0.5f |\" % (i, np.squeeze(costo)))\n",
    "            if i == (num_iter - 1):\n",
    "                print(\"+--------+---------+\")\n",
    "\n",
    "    # Se grafica el costo\n",
    "    plt.plot(np.squeeze(costos))\n",
    "    plt.ylabel(\"costo\")\n",
    "    plt.xlabel(\"iteraciones (x\" + str(Ni) + \")\")\n",
    "    plt.title(\"Evolución del costo (lr = \" + str(learning_rate) + \")\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se definen las constantes que determinan la arquitectura de la red\n",
    "d_0 = X.shape[1]\n",
    "d_1 = 4\n",
    "d_2 = 1\n",
    "dims_capas = [d_0, d_1, d_2]\n",
    "\n",
    "# Se entrena la red, con los parámetros por defecto el costo debería ser alrededor\n",
    "# de 0.69 en la iteración 0 y menor que 0.25 en la 10000\n",
    "inicio = time.time()\n",
    "parametros_red_sinteticos = red_dos_capas_datos_sinteticos(\n",
    "    X, Y, dims_capas=[d_0, d_1, d_2], num_iter=10000, mostrar_costo=True\n",
    ")\n",
    "fin = time.time()\n",
    "print(\"El entrenamiento demoró %.2f segundos.\" % (fin - inicio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Mostrar la frontera de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar la frontera de decisión se deberá completar primero la implementación del método `predecir_clase_datos_sinteticos()`. Dicho método utiliza los parámetros de la red recientemente encontrados para predecir la clase de los vectores de características pasados como parámetro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predecir_clase_datos_sinteticos(X, parametros):\n",
    "    \"\"\"\n",
    "    Esta función predice la clase de los datos sintéticos.\n",
    "\n",
    "    Entrada:\n",
    "        X: matriz de tamaño Nx2 que en cada fila contiene un vector de características.\n",
    "        parametros: parámetros del modelo entrenado.\n",
    "\n",
    "    Salida:\n",
    "        p: vector de tamaño Nx1 que contiene las predicciones realizadas (0 ó 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "\n",
    "    N = X.shape[0]\n",
    "    p = np.zeros((N, 1))\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    # Se hace la propagación hacia adelante de los datos de entrada X. Tener en cuenta que la\n",
    "    # arquitectura utilizada en la red fue Afin --> Tanh --> Afin --> Sigmoide\n",
    "    # Deben poder hacerse en dos líneas de código, o poco más.\n",
    "    # X1, cache1 =\n",
    "    # X2, cache2 =\n",
    "\n",
    "    # Se obtienen las predicciones. Si la salida es mayor que 0.5 se asigna la clase 1, de lo\n",
    "    # contrario se asigna la clase 0.\n",
    "    # Deben poder hacerse en una línea de código, o poco más.\n",
    "    # p = \n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra el porcentaje de acierto con el conjunto de entrenamiento. Verificar que para los parámetros por defecto es mayor al 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicciones_train = predecir_clase_datos_sinteticos(X, parametros_red_sinteticos)\n",
    "porcentaje_aciertos = np.mean(predicciones_train == Y)\n",
    "print(\"El porcentaje de aciertos es %.2f%%.\" % (100 * porcentaje_aciertos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Se muestra la frontera de decisión. Verificar que es razonable para el conjunto de entrenamiento.\n",
    "funcion = lambda x: predecir_clase_datos_sinteticos(x, parametros_red_sinteticos)\n",
    "fuaa.mostrar_frontera_decision(funcion, X, Y.flatten())\n",
    "plt.title(\"Frontera de decisión para una capa oculta de \" + str(d_1) + \" nodos\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Variar la cantidad de nodos utilizados en la capa oculta y comentar sobre la influencia de dicho parámetro en la superficie de decisión obtenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**  \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clasificación de imágenes\n",
    "<img src=\"img/missing_cat.jpg\" style=\"height:250px;border:3px solid;\">\n",
    "<p>Se utilizarán las funciones implementadas anteriormente para distinguir entre imágenes <i>con gatos</i> y <i>sin gatos</i>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda carga los datos. La base cuenta con conjunto de imágenes para entrenamiento y un conjunto para test. En este caso las características a utilizar son directamente los valores de los píxeles de las imágenes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, y_train, features_test, y_test, clases = fuaa.load_cats_dataset()\n",
    "print(\"Se cargaron %d imágenes de entrenamiento: \" % len(features_train), end=\"\")\n",
    "print(\n",
    "    \"%d de la clase %s y \" % (np.sum(y_train == 1), clases[0].decode(\"utf-8\")), end=\"\"\n",
    ")\n",
    "print(\"%d de la clase %s.\" % (np.sum(y_train == 0), clases[1].decode(\"utf-8\")))\n",
    "print(\"Se cargaron %d imágenes de test: \" % len(features_test), end=\"\")\n",
    "print(\"%d de la clase %s y \" % (np.sum(y_test == 1), clases[0].decode(\"utf-8\")), end=\"\")\n",
    "print(\"%d de la clase %s.\" % (np.sum(y_test == 0), clases[1].decode(\"utf-8\")))\n",
    "print(\"Las imágenes son de tamaño [%d, %d, %d].\" % (features_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vez que se ejecuta la siguiente celda se muestran `Nj` ejemplos de cada una de las dos clases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nj = 5\n",
    "indices_gatos = [i for i, label in enumerate(y_train) if label]\n",
    "indices_no_gatos = [i for i, label in enumerate(y_train) if not label]\n",
    "\n",
    "# Ejemplo de una imagen con gato\n",
    "plt.figure(figsize=(15, 15))\n",
    "for k in range(Nj):\n",
    "    pos = np.random.randint(len(indices_gatos))\n",
    "    plt.subplot(1, Nj, k + 1)\n",
    "    plt.imshow(features_train[indices_gatos[pos]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Imagen con gato\")\n",
    "plt.show()\n",
    "\n",
    "# Ejemplo de una imagen sin gato\n",
    "plt.figure(figsize=(15, 15))\n",
    "for k in range(Nj):\n",
    "    pos = np.random.randint(len(indices_no_gatos))\n",
    "    plt.subplot(1, Nj, k + 1)\n",
    "    plt.imshow(features_train[indices_no_gatos[pos]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Imagen sin gato\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.2. Red neuronal de dos capas para clasificar imágenes de gatos\n",
    "\n",
    "**Ejercicio**: Completar la implementación del método `red_dos_capas_clasifica_gatos()` utilizando los métodos *forward* y *backward* adecuados para dicha arquitectura. Como función de costo se utilizará la *entropía cruzada*. La red deberá tener la siguiente estructura: \n",
    "<center><code>Afin --> ReLU --> Afin --> Sigmoide</code></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_dos_capas_clasifica_gatos(\n",
    "    X, Y, dims_capas, num_iter=1000, learning_rate=1, mostrar_costo=False, semilla=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de dos capas: Afin->Relu->Afin->Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        X: datos de entrada, de tamaño (N, d_0).\n",
    "        Y: etiquetas (1 para la clase positiva y 0 para la negativa), de tamaño (N,1).\n",
    "        dims_capas: dimensiones de las capas(d_0, d_1, d_2).\n",
    "        num_iter: número de iteraciones del loop de optimización.\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso por gradiente.\n",
    "        mostrar_costo: Si vale True, se muestra el costo cada 100 iteraciones.\n",
    "        semilla: semilla utilizada para la generación de números aleatorios.\n",
    "    Salida:\n",
    "        parametros: un diccionario de python que contiene W1, W2, b1, and b2.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(semilla)\n",
    "    gradientes = {}  # se inicializa el diccionario que almacena los gradiantes\n",
    "    costos = []  # lista que almacena el costo\n",
    "    N = X.shape[0]  # número de muestras\n",
    "    d_0, d_1, d_2 = dims_capas\n",
    "\n",
    "    # Se inicializan los parámetros del diccionario llamando a una de las\n",
    "    # funciones previamente implementadas\n",
    "    parametros = inicializar_pesos(d_0, d_1, d_2, semilla=semilla)\n",
    "\n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "\n",
    "    # Loop (descenso por gradiente)\n",
    "\n",
    "    for i in range(0, num_iter):\n",
    "\n",
    "        #####################################################\n",
    "        ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "        # Propagación hacia adelante: Afin --> ReLu --> Afin --> Sigmoide.\n",
    "        # Entradas: \"X, W1, b1\". Salidas: \"X1, cache1, X2, cache2\".\n",
    "        # X1, cache1 =\n",
    "        # X2, cache2 =\n",
    "\n",
    "        # Se calcula el costo y se inicia la propagación hacia atrás\n",
    "        # costo, dX2 =\n",
    "\n",
    "        # Propagación hacia atrás.\n",
    "        # Entradas: \"dX2, cache2, cache1\". Salidas: \"dX1, dW2, db2, dW1, db1, dX0 (no utilizado)\".\n",
    "        # dX1, dW2, db2 =\n",
    "        # dX0, dW1, db1 =\n",
    "\n",
    "        # Se almacenan los gradientes recientemente calculados en el diccionario\n",
    "        # gradientes[\"W1\"] =\n",
    "        # gradientes[\"b1\"] =\n",
    "        # gradientes[\"W2\"] =\n",
    "        # gradientes[\"b2\"] =\n",
    "\n",
    "        # Se actualizan los parámetros\n",
    "        # parametros =\n",
    "\n",
    "        # Se obtienen los nuevos W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "        # W1 =\n",
    "        # b1 =\n",
    "        # W2 =\n",
    "        # b2 =\n",
    "\n",
    "        #####################################################\n",
    "        ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "        #####################################################\n",
    "\n",
    "        # Se guarda la evolución del costo cada Ni iteraciones y se muestra cada 10 x Ni.\n",
    "        Ni = 50\n",
    "        if mostrar_costo:\n",
    "            if i == 0:\n",
    "                print(\"+--------+---------+\")\n",
    "                print(\"|  iter  |  costo  |\")\n",
    "                print(\"+--------+---------+\")\n",
    "            if i % Ni == 0:\n",
    "                costos.append(costo)\n",
    "            if i % (10 * Ni) == 0:\n",
    "                print(\"| %6d | %0.5f |\" % (i, np.squeeze(costo)))\n",
    "            if i == (num_iter - 1):\n",
    "                print(\"+--------+---------+\")\n",
    "\n",
    "    # Se grafica el costo\n",
    "    plt.plot(np.squeeze(costos))\n",
    "    plt.ylabel(\"costo\")\n",
    "    plt.xlabel(\"iteraciones (x\" + str(Ni) + \")\")\n",
    "    plt.title(\"Evolución del costo (lr = \" + str(learning_rate) + \")\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las constantes que determinan la arquitectura de la red. En este caso se usarán como características los valores de los píxeles de la imagen. La capa oculta tendrá 7 nodos y habrá un solo nodo en la capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes que definen el modelo\n",
    "d_0 = features_train.shape[1] * features_train.shape[2] * features_train.shape[3]\n",
    "d_1 = 7\n",
    "d_2 = 1\n",
    "dims_capas = [d_0, d_1, d_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace un *reshape* de las imágenes de entrenamiento para que las entradas a la red sean vectores unidimensionales. Además se las normaliza para que los valores estén en el rango [-0.5, 0.5] en vez del rango [0, 255] original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace el reshape de las características\n",
    "feat_train_flat = features_train.reshape(features_train.shape[0], -1)\n",
    "\n",
    "# Se normalizan los datos para que las características queden en el rango [-0.5, 0.5]\n",
    "train_x = feat_train_flat / 255.0 - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar la siguiente celda y encontrar los parámetros del modelo. El costo debería decrecer con el paso de las iteraciones. Verificar que el  costo iteracion en la iteración (`iter`) 0 es aproximadamente 0.69, si no es así se recomienda para la ejecución con el cuadrado (⬛) de la barra superior del notebook y tratar de encontrar el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "parametros_red_gatos = red_dos_capas_clasifica_gatos(\n",
    "    train_x,\n",
    "    y_train,\n",
    "    dims_capas=[d_0, d_1, d_2],\n",
    "    learning_rate=0.1,\n",
    "    num_iter=5001,\n",
    "    mostrar_costo=True,\n",
    ")\n",
    "fin = time.time()\n",
    "print(\"El entrenamiento demoró %.2f segundos\" % (fin - inicio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la función de costo decreció por debajo de 0.003 al acercarse la iteración 5000 entonces la implementación es correcta y has podido entrenar la red neuronal. A continuación veremos qué tan bien funcionan los parámetros encontrados con el conjunto de entrenamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Completar la implementación del método `clasificar_gato()` para que dado un conjunto de imágenes de entrada y los parámetros de la red encontrados durante el entrenamiento devuelva si pertenece a la clase gato o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_gato(X, parametros):\n",
    "    \"\"\"\n",
    "    Esta función predice si las imágenes almacenadas en el vector X son gatos o no\n",
    "\n",
    "    Entrada:\n",
    "        X: matriz de tamaño N x d_0 que contiene N imágenes almacenadas una por fila\n",
    "        parametros: parámetros óptimos encontrados al entrenar la red\n",
    "\n",
    "    Salida:\n",
    "        p: predicciones realizadas por la red para las imágenes en X (1 si es gato, 0 si no)\n",
    "    \"\"\"\n",
    "\n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "\n",
    "    N = X.shape[0]\n",
    "    p = np.zeros((N, 1))\n",
    "\n",
    "    #####################################################\n",
    "    ####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    # Se realiza la propagación hacia adelante de las imágenes de entrada\n",
    "    # Deben poder hacerse en dos líneas de código, o poco más.\n",
    "    # X1, cache1 =\n",
    "    # X2, cache2 =\n",
    "\n",
    "    # Se realiza la predicción\n",
    "    # Deben poder hacerse en una línea de código, o poco más.\n",
    "    # p\n",
    "\n",
    "    #####################################################\n",
    "    ####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "    #####################################################\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar que la red entrenada hace un trabajo fantástico con el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_train = clasificar_gato(train_x, parametros_red_gatos)\n",
    "accuracy_train = 100 * np.mean((predicciones_train == y_train))\n",
    "print(\"Porcentaje de acierto en entrenamiento: %.2f%%.\" % accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cerca del 100% de acierto? ¡Bien hecho! Excelente trabajo. Ahora sólo resta evaluar con el conjunto reservado para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "####### EMPIEZA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "#####################################################\n",
    "\n",
    "# Se hace el reshape de las características\n",
    "\n",
    "# Se normalizan los datos para que las características queden en el rango [-0.5, 0.5]\n",
    "\n",
    "# Clasificar datos de test con la red entrenada\n",
    "\n",
    "# accuracy_test =\n",
    "\n",
    "#####################################################\n",
    "####### TERMINA ESPACIO PARA COMPLETAR CODIGO #######\n",
    "#####################################################\n",
    "\n",
    "print(\"Porcentaje de acierto en test: %.2f%%.\" % accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Mucho mejor que tirar una moneda? Seguro que si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** Comente cuáles son a su criterio las razones que explican el resultado obtenido y comente al menos un par de estrategias que de haberse implementado seguramente habrían redituado en un mejor desempeño con el conjunto de test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Visualizar imágenes del conjunto de test y predicciones correctas y erróneas\n",
    "Mostrar las imágenes del conjunto de test identificando si se cometió error en la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_error = np.where(pred_test != y_test)\n",
    "N_test = len(y_test)\n",
    "n_cols = 6\n",
    "n_rows = int(N_test // n_cols + 1)\n",
    "fig = plt.figure(figsize=(16, 30))\n",
    "for k in range(N_test):\n",
    "    img = features_test[k]\n",
    "\n",
    "    # tsub es el título de cada una de la N_test imágenes que se muestra\n",
    "    # con un color dado por tcolor. Se debe hacer la asignación correcta.\n",
    "    tsub = \"¿Bien/Mal clasificada?\"\n",
    "    tcolor = \"black\"\n",
    "    (tsub, tcolor) = (\n",
    "        (\"Mal clasificada\", \"red\")\n",
    "        if (pred_test[k] != y_test[k])\n",
    "        else (\"Bien clasificada\", \"green\")\n",
    "    )\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, k + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(n_rows, n_cols, k + 1).set_title(tsub, color=tcolor)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Jugando con Tensorflow playground\n",
    "<a id=\"Ejercicio2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 1.** Ejecutar el [modelo parte 1](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=35&networkShape=1&seed=0.68341&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false&discretize_hide=false) y corroborar que no es capaz de separar correctamente los datos. Explicar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 2.** Aumentar el número de neuronas en capa oculta a dos. ¿Se pueden separar los datos? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 3.** Aumentar el número de neuronas en capa oculta a tres utilizando como función de activación ReLU. ¿Es este modelo adecuado para separar los datos? Ejecutar varias veces cambiando la inicialización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 4.** Probar ahora con la siguiente arquitectura:\n",
    "- Primera capa oculta con tres neuronas\n",
    "- Segunda capa oculta con tres neuronas\n",
    "- Tercera capa oculta con dos neuronas\n",
    "\n",
    "¿Este modelo se ajusta mejor a los datos? ¿Converge más rápido y con mayor asiduidad que el de la parte 3? A los efectos de este ejercicio consideraremos que el modelo converge si el error de test es menor a 0.177."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 5.** Utilizando sólamente $X_1$ y $X_2$ como características construir un modelo que separe adecuadamente los datos con [distribución en espiral](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=0&networkShape=1&seed=0.63187&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&showTestData_hide=false&discretize_hide=false). Para ello puede variar libremente el número de capas ocultas y la cantidad de nodos por capa oculta. Indique además los valores elegidos para los siguientes parámetros y las razones que guiaron la elección:\n",
    "- learning rate\n",
    "- activación\n",
    "- tipo de regularización\n",
    "- factor de regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 6.** La extracción de características puede ser útil aún cuando se trabaja con clasificadores de gran capacidad expresiva como las redes neuronales. Utilizando las transformaciones de características a la entrada que considere convenientes, encontrar la arquitectura de red con menor cantidad de nodos que produzca un error con el conjunto de test menor a 0.1. *Sugerencia:*  guarde registro del momento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "564px",
    "width": "450px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "548.117px",
    "left": "10px",
    "top": "127px",
    "width": "294.7px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 451.31666600000005,
   "position": {
    "height": "472.983px",
    "left": "516px",
    "right": "20px",
    "top": "118px",
    "width": "756.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
